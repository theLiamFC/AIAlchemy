{
    "face_detection": "# Face Detection Example\n#\n# This example shows off the built-in face detection feature of the OpenMV Cam.\n#\n# Face detection works by using the Haar Cascade feature detector on an image. A\n# Haar Cascade is a series of simple area contrasts checks. For the built-in\n# frontalface detector there are 25 stages of checks with each stage having\n# hundreds of checks a piece. Haar Cascades run fast because later stages are\n# only evaluated if previous stages pass. Additionally, your OpenMV Cam uses\n# a data structure called the integral image to quickly execute each area\n# contrast check in constant time (the reason for feature detection being\n# grayscale only is because of the space requirment for the integral image).\n\nimport pyb, sensor, time, image, os\n\nprint(os.listdir('/'))\nif 'face-detect' not in os.listdir('/'):\n    os.mkdir('/face-detect')\n\n# Reset sensor\nsensor.reset()\n\n# Sensor settings\nsensor.set_contrast(1)\nsensor.set_gainceiling(16)\n# HQVGA and GRAYSCALE are the best for face tracking.\nsensor.set_framesize(sensor.QVGA)\nsensor.set_pixformat(sensor.GRAYSCALE)\n\nsensor.skip_frames(time = 2000)\n\n# Load Haar Cascade\n# By default this will use all stages, lower satges is faster but less accurate.\nface_cascade = image.HaarCascade('frontalface', stages=25)\nprint(face_cascade)\n\n# FPS clock\nclock = time.clock()\n\nled = pyb.LED(3)\n\ncount = 0\nwhile (True):\n    clock.tick()\n\n    # Capture snapshot\n    img = sensor.snapshot()\n\n    # Find objects.\n    # Note: Lower scale factor scales-down the image more and detects smaller objects.\n    # Higher threshold results in a higher detection rate, with more false positives.\n    objects = img.find_features(face_cascade, threshold=0.75, scale_factor=1.1)\n\n    # Draw objects\n\n    for r in objects:\n        img.draw_rectangle(r)\n        print('%s %s' % (count, r))\n        count = count +1\n\n    if len(objects) > 0:\n\n        #led.on()\n        #time.sleep(50)\n        #led.off()\n        #time.sleep(50)\n        #led.on()\n        #time.sleep(50)\n        #led.off()\n\n        filename = '/face-detect/face-%s' % count\n        print(filename)\n        img.compress(100).save(filename)\n\n    # Print FPS.\n    # Note: Actual FPS is higher, streaming the FB makes it slower.\n    #print(clock.fps())",
    "pyb": {
        "delay": "delay(ms)¶\nDelay for the given number of milliseconds.\n\n",
        "udelay": "udelay(us)¶\nDelay for the given number of microseconds.\n\n",
        "millis": "millis()¶\nReturns the number of milliseconds since the board was last reset.\nThe result is always a MicroPython smallint (31-bit signed number), so\nafter 2^30 milliseconds (about 12.4 days) this will start to return\nnegative numbers.\nNote that if pyb.stop() is issued the hardware counter supporting this\nfunction will pause for the duration of the “sleeping” state. This\nwill affect the outcome of pyb.elapsed_millis().\n\n",
        "micros": "micros()¶\nReturns the number of microseconds since the board was last reset.\nThe result is always a MicroPython smallint (31-bit signed number), so\nafter 2^30 microseconds (about 17.8 minutes) this will start to return\nnegative numbers.\nNote that if pyb.stop() is issued the hardware counter supporting this\nfunction will pause for the duration of the “sleeping” state. This\nwill affect the outcome of pyb.elapsed_micros().\n\n",
        "elapsed_millis": "elapsed_millis(start)¶\nReturns the number of milliseconds which have elapsed since start.\nThis function takes care of counter wrap, and always returns a positive\nnumber. This means it can be used to measure periods up to about 12.4 days.\nExample:\nstart = pyb.millis()\nwhile pyb.elapsed_millis(start) < 1000:\n    # Perform some operation\n\n\n\n",
        "elapsed_micros": "elapsed_micros(start)¶\nReturns the number of microseconds which have elapsed since start.\nThis function takes care of counter wrap, and always returns a positive\nnumber. This means it can be used to measure periods up to about 17.8 minutes.\nExample:\nstart = pyb.micros()\nwhile pyb.elapsed_micros(start) < 1000:\n    # Perform some operation\n    pass\n\n\n\n\n\nReset related functions¶\n",
        "hard_reset": "hard_reset()¶\nResets the OpenMV Cam in a manner similar to pushing the external RESET\nbutton.\n\n",
        "bootloader": "bootloader()¶\nActivate the bootloader without BOOT* pins.\n\n",
        "fault_debug": "fault_debug(value)¶\nEnable or disable hard-fault debugging. A hard-fault is when there is a fatal\nerror in the underlying system, like an invalid memory access.\nIf the value argument is False then the board will automatically reset if\nthere is a hard fault.\nIf value is True then, when the board has a hard fault, it will print the\nregisters and the stack trace, and then cycle the LEDs indefinitely.\nThe default value is disabled, i.e. to automatically reset.\n\n\n\nInterrupt related functions¶\n",
        "disable_irq": "disable_irq()¶\nDisable interrupt requests.\nReturns the previous IRQ state: False/True for disabled/enabled IRQs\nrespectively. This return value can be passed to enable_irq to restore\nthe IRQ to its original state.\n\n",
        "enable_irq": "enable_irq(state=True)¶\nEnable interrupt requests.\nIf state is True (the default value) then IRQs are enabled.\nIf state is False then IRQs are disabled. The most common use of\nthis function is to pass it the value returned by disable_irq to\nexit a critical section.\n\n\n\nPower related functions¶\n",
        "wfi": "wfi()¶\nWait for an internal or external interrupt.\nThis executes a wfi instruction which reduces power consumption\nof the MCU until any interrupt occurs (be it internal or external),\nat which point execution continues. Note that the system-tick interrupt\noccurs once every millisecond (1000Hz) so this function will block for\nat most 1ms.\n\n",
        "stop": "stop()¶\nPut the OpenMV Cam in a “sleeping” state.\nThis reduces power consumption to less than 500 uA. To wake from this\nsleep state requires an external interrupt or a real-time-clock event.\nUpon waking execution continues where it left off.\nSee rtc.wakeup() to configure a real-time-clock wakeup event.\n\n",
        "standby": "standby()¶\nPut the OpenMV Cam into a “deep sleep” state.\nThis reduces power consumption to less than 50 uA. To wake from this\nsleep state requires a real-time-clock event.\nUpon waking the system undergoes a hard reset.\nSee rtc.wakeup() to configure a real-time-clock wakeup event.\n\n\n\nMiscellaneous functions¶\n",
        "have_cdc": "have_cdc()¶\nReturn True if USB is connected as a serial device, False otherwise.\n\nNote\nThis function is deprecated. Use pyb.USB_VCP().isconnected() instead.\n\n\n",
        "hid": "hid((buttons, x, y, z))¶\nTakes a 4-tuple (or list) and sends it to the USB host (the PC) to\nsignal a HID mouse-motion event.\n\nNote\nThis function is deprecated. Use pyb.USB_HID.send() instead.\n\n\n",
        "info": "info([dump_alloc_table])¶\nPrint out lots of information about the board.\n\n",
        "main": "main(filename)¶\nSet the filename of the main script to run after boot.py is finished. If\nthis function is not called then the default file main.py will be executed.\nIt only makes sense to call this function from within boot.py.\n\n",
        "mount": "mount(device, mountpoint, *, readonly=False, mkfs=False)¶\n\nNote\nThis function is deprecated. Mounting and unmounting devices should\nbe performed by os.mount() and os.umount() instead.\n\nMount a block device and make it available as part of the filesystem.\ndevice must be an object that provides the block protocol. (The\nfollowing is also deprecated. See os.AbstractBlockDev for the\ncorrect way to create a block device.)\n\n\nreadblocks(self, blocknum, buf)\nwriteblocks(self, blocknum, buf) (optional)\ncount(self)\nsync(self) (optional)\n\n\nreadblocks and writeblocks should copy data between buf and\nthe block device, starting from block number blocknum on the device.\nbuf will be a bytearray with length a multiple of 512. If\nwriteblocks is not defined then the device is mounted read-only.\nThe return value of these two functions is ignored.\ncount should return the number of blocks available on the device.\nsync, if implemented, should sync the data on the device.\nThe parameter mountpoint is the location in the root of the filesystem\nto mount the device. It must begin with a forward-slash.\nIf readonly is True, then the device is mounted read-only,\notherwise it is mounted read-write.\nIf mkfs is True, then a new filesystem is created if one does not\nalready exist.\n\n",
        "repl_uart": "repl_uart(uart)¶\nGet or set the UART object where the REPL is repeated on.\n\n",
        "rng": "rng()¶\nReturn a 30-bit hardware generated random number.\n\n",
        "sync": "sync()¶\nSync all file systems.\n\n",
        "unique_id": "unique_id()¶\nReturns a string of 12 bytes (96 bits), which is the unique ID of the MCU.\n\n",
        "usb_mode": "usb_mode([modestr, ]port=-1, vid=0xf055, pid=-1, msc=(), hid=pyb.hid_mouse, high_speed=False)¶\nIf called with no arguments, return the current USB mode as a string.\nIf called with modestr provided, attempts to configure the USB mode.\nThe following values of modestr are understood:\n\nNone: disables USB\n'VCP': enable with VCP (Virtual COM Port) interface\n'MSC': enable with MSC (mass storage device class) interface\n'VCP+MSC': enable with VCP and MSC\n'VCP+HID': enable with VCP and HID (human interface device)\n'VCP+MSC+HID': enabled with VCP, MSC and HID (only available on PYBD boards)\n\nFor backwards compatibility, 'CDC' is understood to mean\n'VCP' (and similarly for 'CDC+MSC' and 'CDC+HID').\nThe port parameter should be an integer (0, 1, …) and selects which\nUSB port to use if the board supports multiple ports. A value of -1 uses\nthe default or automatically selected port.\nThe vid and pid parameters allow you to specify the VID (vendor id)\nand PID (product id). A pid value of -1 will select a PID based on the\nvalue of modestr.\nIf enabling MSC mode, the msc parameter can be used to specify a list\nof SCSI LUNs to expose on the mass storage interface. For example\nmsc=(pyb.Flash(), pyb.SDCard()).\nIf enabling HID mode, you may also specify the HID details by\npassing the hid keyword parameter. It takes a tuple of\n(subclass, protocol, max packet length, polling interval, report\ndescriptor). By default it will set appropriate values for a USB\nmouse. There is also a pyb.hid_keyboard constant, which is an\nappropriate tuple for a USB keyboard.\nThe high_speed parameter, when set to True, enables USB HS mode if\nit is supported by the hardware.\n\n\n\nConstants¶\n",
        "hid_mouse¶": "hid_mouse¶"
    },
    "stm": {
        "rfcore_status": "rfcore_status()¶\nReturns the status of the second CPU as an integer (the first word of device\ninfo table).\n\n",
        "rfcore_fw_version": "rfcore_fw_version(id)¶\nGet the version of the firmware running on the second CPU. Pass in 0 for\nid to get the FUS version, and 1 to get the WS version.\nReturns a 5-tuple with the full version number.\n\n",
        "rfcore_sys_hci": "rfcore_sys_hci(ogf, ocf, data, timeout_ms=0)¶\nExecute a HCI command on the SYS channel. The execution is synchronous.\nReturns a bytes object with the result of the SYS command.\n\n\n\nFunctions specific to STM32WLxx MCUs¶\nThese functions are available on STM32WLxx microcontrollers, and interact with\nthe integrated “SUBGHZ” radio modem peripheral.\n",
        "subghz_cs": "subghz_cs(level)¶\nSets the internal SPI CS pin attached to the radio peripheral. The level\nargument is active-low: a truthy value means “CS pin high” and de-asserts the\nsignal, a falsey value means “CS pin low” and asserts the signal.\nThe internal-only SPI bus corresponding to this CS signal can be instantiated\nusing machine.SPI() id value \"SUBGHZ\".\n\n",
        "subghz_irq": "subghz_irq(handler)¶\nSets the internal SUBGHZ radio interrupt handler to the provided\nfunction. The handler function is called as a “hard” interrupt in response to\nradio peripheral interrupts. See Writing interrupt handlers for more information about\ninterrupt handlers in MicroPython.\nCalling this function with the handler argument set to None disables the IRQ.\nDue to a hardware limitation, each time this IRQ fires MicroPython disables\nit before calling the handler. In order to receive another interrupt, Python\ncode should call subghz_irq() to set the handler again. This has the side\neffect of re-enabling the IRQ.\n\n",
        "subghz_is_busy": "subghz_is_busy()¶\nReturn a bool corresponding to the internal “RFBUSYS” signal from the\nradio peripheral. Before sending a new command to the radio over SPI then\nthis function should be polled until it returns False, to confirm the\nbusy signal is de-asserted.\n\n\n\n\n\n\n Previous\nNext \n\n\n\n© Copyright - The MicroPython Documentation is Copyright © 2014-2024, Damien P. George, Paul Sokolovsky, and contributors.\nLast updated on 05 Mar 2024.\n\n\nBuilt with Sphinx using a\ntheme\nprovided by Read the Docs.\n\n\n\n\n\n\n\n Language and External Links\n\n\n\n\nLanguage\n\nEnglish\n\n\n中文\n\n\n\n\nExternal links\n\nopenmv.io\n\n\nforums.openmv.io\n\n\ngithub.com/openmv/openmv\n\n\nmicropython.org\n\n\nforum.micropython.org\n\n\ngithub.com/micropython/micropython\n\n\n\n\n\n"
    },
    "sensor": {
        "reset": "reset()¶\nInitializes the camera sensor.\n\n",
        "sleep": "sleep(enable)¶\nPuts the camera to sleep if enable is True. Otherwise, wakes it back up.\n\n",
        "shutdown": "shutdown(enable)¶\nPuts the camera into a lower power mode than sleep (but the camera must be reset on being woken up).\n\n",
        "flush": "flush()¶\nCopies whatever was in the frame buffer to the IDE. You should call this\nmethod to display the last image your OpenMV Cam takes if it’s not running\na script with an infinite loop. Note that you’ll need to add a delay time\nof about a second after your script finishes for the IDE to grab the image\nfrom your camera. Otherwise, this method will have no effect.\n\n",
        "snapshot": "snapshot()¶\nTakes a picture using the camera and returns an image object.\nThe OpenMV Cam has two memory areas for images. The classical stack/heap\narea used for normal MicroPython processing can store small images within\nit’s heap. However, the MicroPython heap is only about ~100 KB which is not\nenough to store larger images. So, your OpenMV Cam has a secondary frame\nbuffer memory area that stores images taken by sensor.snapshot(). Images\nare stored on the bottom of this memory area. Any memory that’s left\nover is then available for use by the frame buffer stack which your OpenMV\nCam’s firmware uses to hold large temporary data structures for image\nprocessing algorithms.\nIf you need room to hold multiple frames you may “steal” frame buffer space\nby calling sensor.alloc_extra_fb().\nIf sensor.set_auto_rotation() is enabled this method will return a new\nalready rotated image object.\n\nNote\nsensor.snapshot() may apply cropping parameters to fit the snapshot in the available\nRAM the pixformat, framesize, windowing, and framebuffers. The cropping parameters will be applied\nto maintain the aspect ratio and will stay until sensor.set_framesize() or sensor.set_windowing() are called.\n\n\n",
        "skip_frames": "skip_frames([n, time])¶\nTakes n number of snapshots to let the camera image stabilize after\nchanging camera settings. n is passed as normal argument, e.g.\nskip_frames(10) to skip 10 frames. You should call this function after\nchanging camera settings.\nAlternatively, you can pass the keyword argument time to skip frames\nfor some number of milliseconds, e.g. skip_frames(time = 2000) to skip\nframes for 2000 milliseconds.\nIf neither n nor time is specified this method skips frames for\n300 milliseconds.\nIf both are specified this method skips n number of frames but will\ntimeout after time milliseconds.\n\nNote\nsensor.snapshot() may apply cropping parameters to fit the snapshot in the available\nRAM given the pixformat, framesize, windowing, and framebuffers. The cropping parameters will be applied\nto maintain the aspect ratio and will stay until sensor.set_framesize() or sensor.set_windowing() are called.\n\n\n",
        "width": "width()¶\nReturns the sensor resolution width.\n\n",
        "height": "height()¶\nReturns the sensor resolution height.\n\n",
        "get_fb": "get_fb()¶\n(Get Frame Buffer) Returns the image object returned by a previous call of\nsensor.snapshot(). If sensor.snapshot() had not been called before\nthen None is returned.\n\n",
        "get_id": "get_id()¶\nReturns the camera module ID.\n",
        "alloc_extra_fb": "alloc_extra_fb(width, height, pixformat)¶\nAllocates another frame buffer for image storage from the frame buffer stack\nand returns an image object of width, height, and pixformat.\nYou may call this function as many times as you like as long as there’s\nmemory available to allocate any number of extra frame buffers.\nIf pixformat is a number >= 4 then this will allocate a JPEG image. You\ncan then do Image.bytearray() to get byte level read/write access to the JPEG image.\n\nNote\nCreating secondary images normally requires creating them on the heap which\nhas a limited amount of RAM… but, also gets fragmented making it hard to\ngrab a large contigous memory array to store an image in. With this method\nyou are able to allocate a very large memory array for an image instantly\nby taking space away from our frame buffer stack memory which we use for\ncomputer vision algorithms. That said, this also means you’ll run out of\nmemory more easily if you try to execute more memory intensive machine\nvision algorithms like Image.find_apriltags().\n\n\n",
        "dealloc_extra_fb": "dealloc_extra_fb()¶\nDeallocates the last previously allocated extra frame buffer. Extra frame\nbuffers are stored in a stack like structure.\n\nNote\nYour OpenMV Cam has two memory areas. First, you have your classical\n.data/.bss/heap/stack memory area. The .data/.bss/heap regions are\nfixed by firmware. The stack then grows down until it hits the heap.\nNext, frame buffers are stored in a secondary memory region. Memory is\nliad out with the main frame buffer on the bottom and the frame buffer\nstack on the top. When sensor.snapshot() is called it fills the frame bufer\nfrom the bottom. The frame buffer stack is then able to use whatever is\nleft over. This memory allocation method is extremely efficent for computer\nvision on microcontrollers.\n\n\n",
        "set_pixformat": "set_pixformat(pixformat)¶\nSets the pixel format for the camera module.\n",
        "get_pixformat": "get_pixformat()¶\nReturns the pixformat for the camera module.\n\n",
        "set_framesize": "set_framesize(framesize)¶\nSets the frame size for the camera module.\n",
        "get_framesize": "get_framesize()¶\nReturns the frame size for the camera module.\n\n",
        "set_framerate": "set_framerate(rate)¶\nSets the frame rate in hz for the camera module.\n\nNote\nset_framerate works by dropping frames received by the camera module to keep the frame rate\nequal to (or below) the rate you specify. By default the camera will run at the maximum frame\nrate. If implemented for the particular camera sensor then set_framerate will also reduce\nthe camera sensor frame rate internally to save power and improve image quality by increasing\nthe sensor exposure. set_framerate may conflict with set_auto_exposure on some cameras.\n\n\n",
        "get_framerate": "get_framerate()¶\nReturns the frame rate in hz for the camera module.\n\n",
        "set_windowing": "set_windowing(roi)¶\nSets the resolution of the camera to a sub resolution inside of the current\nresolution. For example, setting the resolution to sensor.VGA and then\nthe windowing to (120, 140, 200, 200) sets sensor.snapshot() to capture\nthe 200x200 center pixels of the VGA resolution outputted by the camera\nsensor. You can use windowing to get custom resolutions. Also, when using\nwindowing on a larger resolution you effectively are digital zooming.\nroi is a rect tuple (x, y, w, h). However, you may just pass (w, h) and\nthe roi will be centered on the frame. You may also pass roi not in parens.\nThis function will automatically handle cropping the passed roi to the framesize.\n\n",
        "get_windowing": "get_windowing()¶\nReturns the roi tuple (x, y, w, h) previously set with sensor.set_windowing().\n\n",
        "set_gainceiling": "set_gainceiling(gainceiling)¶\nSet the camera image gainceiling. 2, 4, 8, 16, 32, 64, or 128.\n\n",
        "set_contrast": "set_contrast(constrast)¶\nSet the camera image contrast. -3 to +3.\n\n",
        "set_brightness": "set_brightness(brightness)¶\nSet the camera image brightness. -3 to +3.\n\n",
        "set_saturation": "set_saturation(saturation)¶\nSet the camera image saturation. -3 to +3.\n\n",
        "set_quality": "set_quality(quality)¶\nSet the camera image JPEG compression quality. 0 - 100.\n\nNote\nOnly for the OV2640/OV5640 cameras.\n\n\n",
        "set_colorbar": "set_colorbar(enable)¶\nTurns color bar mode on (True) or off (False). Defaults to off.\n\n",
        "set_auto_gain": "set_auto_gain(enable[, gain_db=-1[, gain_db_ceiling]])¶\nenable turns auto gain control on (True) or off (False).\nThe camera will startup with auto gain control on.\nIf enable is False you may set a fixed gain in decibels with gain_db.\nIf enable is True you may set the maximum gain ceiling in decibels with\ngain_db_ceiling for the automatic gain control algorithm.\n\nNote\nYou need to turn off white balance too if you want to track colors.\n\n\n",
        "get_gain_db": "get_gain_db()¶\nReturns the current camera gain value in decibels (float).\n\n",
        "set_auto_exposure": "set_auto_exposure(enable[, exposure_us])¶\nenable turns auto exposure control on (True) or off (False).\nThe camera will startup with auto exposure control on.\nIf enable is False you may set a fixed exposure time in microseconds\nwith exposure_us.\n\nNote\nCamera auto exposure algorithms are pretty conservative about how much\nthey adjust the exposure value by and will generally avoid changing the\nexposure value by much. Instead, they change the gain value alot of deal\nwith changing lighting.\n\n\n",
        "get_exposure_us": "get_exposure_us()¶\nReturns the current camera exposure value in microseconds (int).\n\n",
        "set_auto_whitebal": "set_auto_whitebal(enable[, rgb_gain_db])¶\nenable turns auto white balance on (True) or off (False).\nThe camera will startup with auto white balance on.\nIf enable is False you may set a fixed gain in decibels for the red, green,\nand blue channels respectively with rgb_gain_db.\n\nNote\nYou need to turn off gain control too if you want to track colors.\n\n\n",
        "get_rgb_gain_db": "get_rgb_gain_db()¶\nReturns a tuple with the current camera red, green, and blue gain values in\ndecibels ((float, float, float)).\n\n",
        "set_auto_blc": "set_auto_blc([enable[, regs]])¶\nSets the auto black line calibration (blc) control on the camera.\nenable pass True or False to turn BLC on or off. You typically always want this on.\nregs if disabled then you can manually set the blc register values via the values you\ngot previously from get_blc_regs().\n\n",
        "get_blc_regs": "get_blc_regs()¶\nReturns the sensor blc registers as an opaque tuple of integers. For use with set_auto_blc.\n\n",
        "set_hmirror": "set_hmirror(enable)¶\nTurns horizontal mirror mode on (True) or off (False). Defaults to off.\n\n",
        "get_hmirror": "get_hmirror()¶\nReturns if horizontal mirror mode is enabled.\n\n",
        "set_vflip": "set_vflip(enable)¶\nTurns vertical flip mode on (True) or off (False). Defaults to off.\n\n",
        "get_vflip": "get_vflip()¶\nReturns if vertical flip mode is enabled.\n\n",
        "set_transpose": "set_transpose(enable)¶\nTurns transpose mode on (True) or off (False). Defaults to off.\n\n\nvflip=False, hmirror=False, transpose=False -> 0 degree rotation\nvflip=True, hmirror=False, transpose=True -> 90 degree rotation\nvflip=True, hmirror=True, transpose=False -> 180 degree rotation\nvflip=False, hmirror=True, transpose=True -> 270 degree rotation\n\n\n\n",
        "get_transpose": "get_transpose()¶\nReturns if transpose mode is enabled.\n\n",
        "set_auto_rotation": "set_auto_rotation(enable)¶\nTurns auto rotation mode on (True) or off (False). Defaults to off.\n\nNote\nThis function only works when the OpenMV Cam has an imu installed and is enabled automatically.\n\n\n",
        "get_auto_rotation": "get_auto_rotation()¶\nReturns if auto rotation mode is enabled.\n\nNote\nThis function only works when the OpenMV Cam has an imu installed and is enabled automatically.\n\n\n",
        "set_framebuffers": "set_framebuffers(count)¶\nSets the number of frame buffers used to receive image data. By default your OpenMV Cam will\nautomatically try to allocate the maximum number of frame buffers it can possibly allocate\nwithout using more than 1/2 of the available frame buffer RAM at the time of allocation to\nensure the best performance. Automatic reallocation of frame buffers occurs whenever you\ncall sensor.set_pixformat(), sensor.set_framesize(), and sensor.set_windowing().\nsensor.snapshot() will automatically handle switching active frame buffers in the background.\nFrom your code’s perspective there is only ever 1 active frame buffer even though there might\nbe more than 1 frame buffer on the system and another frame buffer reciving data in the background.\nIf count is:\n\n\n1 - Single Buffer Mode (you may also pass sensor.SINGLE_BUFFER)In single buffer mode your OpenMV Cam will allocate one frame buffer for receiving images.\nWhen you call sensor.snapshot() that framebuffer will be used to receive the image and\nthe camera driver will continue to run. In the advent you call sensor.snapshot() again\nbefore the first line of the next frame is received your code will execute at the frame rate\nof the camera. Otherwise, the image will be dropped.\n\n2 - Double Buffer Mode (you may also pass sensor.DOUBLE_BUFFER)In double buffer mode your OpenMV Cam will allocate two frame buffers for receiving images.\nWhen you call sensor.snapshot() one framebuffer will be used to receive the image and\nthe camera driver will continue to run. When the next frame is received it will be stored\nin the other frame bufer. In the advent you call sensor.snapshot() again\nbefore the first line of the next frame after is received your code will execute at the frame rate\nof the camera. Otherwise, the image will be dropped.\n\n3 - Triple Buffer Mode (you may also pass sensor.TRIPLE_BUFFER)In triple buffer mode your OpenMV Cam will allocate three buffers for receiving images.\nIn this mode there is always a frame buffer to store the received image to in the background\nresulting in the highest performance and lowest latency for reading the latest received frame.\nNo frames are ever dropped in this mode. The next frame read by sensor.snapshot() is the\nlast captured frame by the sensor driver (e.g. if you are reading slower than the camera\nframe rate then the older frame in the possible frames available is skipped).\n\n\n\nRegarding the reallocation above, triple buffering is tried first, then double buffering, and if\nthese both fail to fit in 1/2 of the available frame buffer RAM then single buffer mode is used.\nYou may pass a value of 4 or greater to put the sensor driver into video FIFO mode where received\nimages are stored in a frame buffer FIFO with count buffers. This is useful for video recording\nto an SD card which may randomly block your code from writing data when the SD card is performing\nhouse-keeping tasks like pre-erasing blocks to write data to.\n\nNote\nOn frame drop (no buffers available to receive the next frame) all frame buffers are automatically\ncleared except the active frame buffer. This is done to ensure sensor.snapshot() returns current\nframes and not frames from long ago.\n\nFun fact, you can pass a value of 100 or so on OpenMV Cam’s with SDRAM for a huge video fifo. If\nyou then call snapshot slower than the camera frame rate (by adding machine.sleep()) you’ll get\nslow-mo effects in OpenMV IDE. However, you will also see the above policy effect of resetting\nthe frame buffer on a frame drop to ensure that frames do not get too old. If you want to record\nslow-mo video just record video normally to the SD card and then play the video back on a desktop\nmachine slower than it was recorded.\n\n",
        "get_framebuffers": "get_framebuffers()¶\nReturns the current number of frame buffers allocated.\n\n",
        "disable_delays": "disable_delays([disable])¶\nIf disable is True then disable all settling time delays in the sensor module.\nWhenever you reset the camera module, change modes, etc. the sensor driver delays to prevent\nyou can from calling snapshot to quickly afterwards and receiving corrupt frames from the\ncamera module. By disabling delays you can quickly update the camera module settings in bulk\nvia multiple function calls before delaying at the end and calling snapshot.\nIf this function is called with no arguments it returns if delays are disabled.\n\n",
        "disable_full_flush": "disable_full_flush([disable])¶\nIf disable is True then automatic framebuffer flushing mentioned in set_framebuffers\nis disabled. This removes any time limit on frames in the frame buffer fifo. For example, if\nyou set the number of frame buffers to 30 and set the frame rate to 30 you can now precisely\nrecord 1 second of video from the camera without risk of frame loss.\nIf this function is called with no arguments it returns if automatic flushing is disabled. By\ndefault automatic flushing on frame drop is enabled to clear out stale frames.\n\nNote\nsnapshot starts the frame capture process which will continue to capture frames until\nthere is no space to hold a frame at which point the frame capture process stops. The\nprocess always stops when there is no space to hold the next frame.\n\n\n",
        "set_lens_correction": "set_lens_correction(enable, radi, coef)¶\nenable True to enable and False to disable (bool).\nradi integer radius of pixels to correct (int).\ncoef power of correction (int).\n\n",
        "set_vsync_callback": "set_vsync_callback(cb)¶\nRegisters callback cb to be executed (in interrupt context) whenever the camera module\ngenerates a new frame (but, before the frame is received).\ncb takes one argument and is passed the current state of the vsync pin after changing.\n\n",
        "set_frame_callback": "set_frame_callback(cb)¶\nRegisters callback cb to be executed (in interrupt context) whenever the camera module\ngenerates a new frame and the frame is ready to be read via sensor.snapshot().\ncb takes no arguments.\nUse this to get an interrupt to schedule reading a frame later with micropython.schedule().\n\n",
        "get_frame_available": "get_frame_available()¶\nReturns True if a frame is available to read by calling sensor.snapshot().\n\n",
        "ioctl": "ioctl(...)¶\nExecutes a sensor specific method:\n",
        "set_color_palette": "set_color_palette(palette)¶\nSets the color palette to use for FLIR Lepton grayscale to RGB565 conversion.\n\n",
        "get_color_palette": "get_color_palette()¶\nReturns the current color palette setting. Defaults to image.PALETTE_RAINBOW.\n\n",
        "__write_reg": "__write_reg(address, value)¶\nWrite value (int) to camera register at address (int).\n\nNote\nSee the camera data sheet for register info.\n\n\n",
        "__read_reg": "__read_reg(address)¶\nRead camera register at address (int).\n\nNote\nSee the camera data sheet for register info.\n\n\n\n\nConstants¶\n",
        "BINARY¶\nBINARY ": "BINARY¶\nBINARY (bitmap) pixel format. Each pixel is 1-bit.\nThis format is usful for mask storage. Can be used with image.Image() and\nsensor.alloc_extra_fb().\n\n"
    },
    "image": {
        "binary_to_grayscale": "binary_to_grayscale(binary_image_value)¶\nReturns a converted binary value (0-1) to a grayscale value (0-255).\n\n",
        "binary_to_rgb": "binary_to_rgb(binary_image_value)¶\nReturns a converted binary value (0-1) to a 3 value RGB888 tuple.\n\n",
        "binary_to_lab": "binary_to_lab(binary_image_value)¶\nReturns a converted binary value (0-1) to a 3 value LAB tuple.\nL goes between 0 and 100 and A/B go from -128 to 128.\n\n",
        "binary_to_yuv": "binary_to_yuv(binary_image_value)¶\nReturns a converted binary value (0-1) to a 3 value YUV tuple.\nY goes between 0 and 255 and U/V go from -128 to 128.\n\n",
        "grayscale_to_binary": "grayscale_to_binary(grayscale_value)¶\nReturns a converted grayscale value (0-255) to a binary value (0-1).\n\n",
        "grayscale_to_rgb": "grayscale_to_rgb(grayscale_value)¶\nReturns a converted grayscale value to a 3 value RGB888 tuple.\n\nNote\nThe OpenMV Cam firmware does the conversion using a RGB565->RGB888 process\nso this method won’t return the exact values as a pure RGB888 system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "grayscale_to_lab": "grayscale_to_lab(grayscale_value)¶\nReturns a converted grayscale value to a 3 value LAB tuple.\nL goes between 0 and 100 and A/B go from -128 to 128.\n\nNote\nThe OpenMV Cam firmware does the conversion using a RGB565->LAB process\nso this method won’t return the exact values as a pure LAB system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "grayscale_to_yuv": "grayscale_to_yuv(grayscale_value)¶\nReturns a converted grayscale value to a 3 value YUV tuple.\nY goes between 0 and 255 and U/V go from -128 to 128.\n\nNote\nThe OpenMV Cam firmware does the conversion using a RGB565->YUV process\nso this method won’t return the exact values as a pure YUV system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "rgb_to_binary": "rgb_to_binary(rgb_tuple)¶\nReturns a converted 3 value RGB888 tuple to a center range thresholded binary value (0-1).\n\nNote\nThe OpenMV Cam firmware does the conversion using a RGB888->RGB565 process\nso this method won’t return the exact values as a pure RGB888 system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "rgb_to_grayscale": "rgb_to_grayscale(rgb_tuple)¶\nReturns a converted 3 value RGB888 tuple to a grayscale value (0-255).\n\nNote\nThe OpenMV Cam firmware does the conversion using a RGB888->RGB565 process\nso this method won’t return the exact values as a pure RGB888 system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "rgb_to_lab": "rgb_to_lab(rgb_tuple)¶\nReturns a converted 3 value RGB888 tuple to a 3 value LAB tuple.\nL goes between 0 and 100 and A/B go from -128 to 128.\n\nNote\nThe OpenMV Cam firmware does the conversion using a RGB888->RGB565 process\nso this method won’t return the exact values as a pure RGB888 system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "rgb_to_yuv": "rgb_to_yuv(rgb_tuple)¶\nReturns a converted 3 value RGB888 tuple to a 3 value YUV tuple.\nY goes between 0 and 255 and U/V go from -128 to 128.\n\nNote\nThe OpenMV Cam firmware does the conversion using a RGB888->RGB565 process\nso this method won’t return the exact values as a pure RGB888 system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "lab_to_binary": "lab_to_binary(lab_tuple)¶\nReturns a converted 3 value LAB tuple to a center range thresholded binary value (0-1).\n\nNote\nThe OpenMV Cam firmware does the conversion using a LAB->RGB565 process\nso this method won’t return the exact values as a pure LAB system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "lab_to_grayscale": "lab_to_grayscale(lab_tuple)¶\nReturns a converted 3 value LAB tuple to a grayscale value (0-255).\n\nNote\nThe OpenMV Cam firmware does the conversion using a LAB->RGB565 process\nso this method won’t return the exact values as a pure LAB system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "lab_to_rgb": "lab_to_rgb(lab_tuple)¶\nReturns a converted 3 value LAB tuple to a 3 value RGB888 tuple.\n\nNote\nThe OpenMV Cam firmware does the conversion using a LAB->RGB565 process\nso this method won’t return the exact values as a pure LAB system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "lab_to_yuv": "lab_to_yuv(lab_tuple)¶\nReturns a converted 3 value LAB tuple to a 3 value YUV tuple.\nY goes between 0 and 255 and U/V go from -128 to 128.\n\nNote\nThe OpenMV Cam firmware does the conversion using a LAB->RGB565 process\nso this method won’t return the exact values as a pure LAB system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "yuv_to_binary": "yuv_to_binary(yuv_tuple)¶\nReturns a converted 3 value YUV tuple to a center range thresholded binary value (0-1).\n\nNote\nThe OpenMV Cam firmware does the conversion using a YUV->RGB565 process\nso this method won’t return the exact values as a pure YUV system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "yuv_to_grayscale": "yuv_to_grayscale(yuv_tuple)¶\nReturns a converted 3 value YUV tuple to a grayscale value (0-255).\n\nNote\nThe OpenMV Cam firmware does the conversion using a YUV->RGB565 process\nso this method won’t return the exact values as a pure YUV system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "yuv_to_rgb": "yuv_to_rgb(lab_tuple)¶\nReturns a converted 3 value YUV tuple to a 3 value RGB888 tuple.\n\nNote\nThe OpenMV Cam firmware does the conversion using a YUV->RGB565 process\nso this method won’t return the exact values as a pure YUV system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "yuv_to_lab": "yuv_to_lab(yuv_tuple)¶\nReturns a converted 3 value YUV tuple to a 3 value LAB tuple.\nL goes between 0 and 100 and A/B go from -128 to 128.\n\nNote\nThe OpenMV Cam firmware does the conversion using a YUV->RGB565 process\nso this method won’t return the exact values as a pure YUV system would.\nHowever, it’s true to how the image lib works internally.\n\n\n",
        "load_decriptor": "load_decriptor(path)¶\nLoads a descriptor object from disk.\npath is the path to the descriptor file to load.\n\n",
        "save_descriptor": "save_descriptor(path, descriptor)¶\nSaves the descriptor object descriptor to disk.\npath is the path to the descriptor file to save.\n\n",
        "match_descriptor": "match_descriptor(descritor0, descriptor1[, threshold=70[, filter_outliers=False]])¶\nFor LBP descriptors this function returns an integer representing the\ndifference between the two descriptors. You may then threshold/compare this\ndistance metric as necessary. The distance is a measure of similarity. The\ncloser it is to zero the better the LBP keypoint match.\nFor ORB descriptors this function returns the kptmatch object. See above.\nthreshold is used for ORB keypoints to filter ambiguous matches. A lower\nthreshold value tightens the keypoint matching algorithm. threshold\nmay be between 0-100 (int). Defaults to 70.\nfilter_outliers is used for ORB keypoints to filter out outlier\nkeypoints allow you to raise the threshold. Defaults to False.\n\n\n\nclass HaarCascade – Feature Descriptor¶\nThe Haar Cascade feature descriptor is used for the Image.find_features()\nmethod. It doesn’t have any methods itself for you to call.\n\n\nclass image.HaarCascade(path[, stages=Auto])¶\nLoads a Haar Cascade into memory from a Haar Cascade binary file formatted\nfor your OpenMV Cam. If you pass “frontalface” instead of a path then this\nconstructor will load the built-in frontal face Haar Cascade into memory.\nAdditionally, you can also pass “eye” to load a Haar Cascade for eyes into\nmemory. Finally, this method returns the loaded Haar Cascade object for use\nwith Image.find_features().\nstages defaults to the number of stages in the Haar Cascade. However,\nyou can specify a lower number of stages to speed up processing the feature\ndetector at the cost of a higher rate of false positives.\n\nNote\nYou can make your own Haar Cascades to use with your OpenMV Cam.\nFirst, Google for “<thing> Haar Cascade” to see if someone\nalready made an OpenCV Haar Cascade for an object you want to\ndetect. If not… then you’ll have to generate your own (which is\na lot of work). See here\nfor how to make your own Haar Cascade. Then see this script\nfor converting OpenCV Haar Cascades into a format your OpenMV Cam\ncan read.\n\nQ: What is a Haar Cascade?\nA: A Haar Cascade is a series of contrast checks that are used to determine\nif an object is present in the image. The contrast checks are split of into\nstages where a stage is only run if previous stages have already passed.\nThe contrast checks are simple things like checking if the center vertical\nof the image is lighter than the edges. Large area checks are performed\nfirst in the earlier stages followed by more numerous and smaller area\nchecks in later stages.\nQ: How are Haar Cascades made?\nA: Haar Cascades are made by training the generator algorithm against\npositive and negative labeled images. For example, you’d train the\ngenerator algorithm against hundreds of pictures with cats in them that\nhave been labeled as images with cats and against hundreds of images with\nnot cat like things labeled differently. The generator algorithm will then\nproduce a Haar Cascade that detects cats.\n\n\n\nclass Histogram – Histogram Object¶\nThe histogram object is returned by Image.get_histogram().\nGrayscale histograms have one channel with some number of bins. All bins are\nnormalized so that all bins sum to 1.\nRGB565 histograms have three channels with some number of bins each. All bins\nare normalized so that all bins in a channel sum to 1.\n\n\nclass image.histogram¶\nPlease call Image.get_histogram() to create this object.\n\n\nbins()¶\nReturns a list of floats for the grayscale histogram.\nYou may also get this value doing [0] on the object.\n\n\n\nl_bins()¶\nReturns a list of floats for the RGB565 histogram LAB L channel.\nYou may also get this value doing [0] on the object.\n\n\n\na_bins()¶\nReturns a list of floats for the RGB565 histogram LAB A channel.\nYou may also get this value doing [1] on the object.\n\n\n\nb_bins()¶\nReturns a list of floats for the RGB565 histogram LAB B channel.\nYou may also get this value doing [2] on the object.\n\n\n\nget_percentile(percentile)¶\nComputes the CDF of the histogram channels and returns a image.percentile\nobject with the values of the histogram at the passed in percentile (0.0\n- 1.0) (float). So, if you pass in 0.1 this method will tell you (going from\nleft-to-right in the histogram) what bin when summed into an accumulator\ncaused the accumulator to cross 0.1. This is useful to determine min (with\n0.1) and max (with 0.9) of a color distribution without outlier effects\nruining your results for adaptive color tracking.\n\n\n\nget_threshold()¶\nUses Otsu’s Method to compute the optimal threshold values that split the\nhistogram into two halves for each channel of the histogram. This method\nreturns a image.threshold object. This method is particularly useful for\ndetermining optimal Image.binary() thresholds.\n\n\n\nget_statistics()¶\nComputes the mean, median, mode, standard deviation, min, max, lower\nquartile, and upper quartile of each color channel in the histogram and\nreturns a statistics object.\nYou may also use histogram.statistics() and histogram.get_stats()\nas aliases for this method.\n\n\n\n\nclass Percentile – Percentile Object¶\nThe percentile object is returned by histogram.get_percentile().\nGrayscale percentiles have one channel. Use the non l_*, a_*, and\nb_* method.\nRGB565 percentiles have three channels. Use the l_*, a_*, and b_*\nmethods.\n\n\nclass image.percentile¶\nPlease call histogram.get_percentile() to create this object.\n\n\nvalue()¶\nReturn the grayscale percentile value (between 0 and 255).\nYou may also get this value doing [0] on the object.\n\n\n\nl_value()¶\nReturn the RGB565 LAB L channel percentile value (between 0 and 100).\nYou may also get this value doing [0] on the object.\n\n\n\na_value()¶\nReturn the RGB565 LAB A channel percentile value (between -128 and 127).\nYou may also get this value doing [1] on the object.\n\n\n\nb_value()¶\nReturn the RGB565 LAB B channel percentile value (between -128 and 127).\nYou may also get this value doing [2] on the object.\n\n\n\n\nclass Threshold – Threshold Object¶\nThe threshold object is returned by histogram.get_threshold().\nGrayscale thresholds have one channel. Use the non l_*, a_*, and\nb_* method.\nRGB565 thresholds have three channels. Use the l_*, a_*, and b_*\nmethods.\n\n\nclass image.threshold¶\nPlease call histogram.get_threshold() to create this object.\n\n\nvalue()¶\nReturn the grayscale threshold value (between 0 and 255).\nYou may also get this value doing [0] on the object.\n\n\n\nl_value()¶\nReturn the RGB565 LAB L channel threshold value (between 0 and 100).\nYou may also get this value doing [0] on the object.\n\n\n\na_value()¶\nReturn the RGB565 LAB A channel threshold value (between -128 and 127).\nYou may also get this value doing [1] on the object.\n\n\n\nb_value()¶\nReturn the RGB565 LAB B channel threshold value (between -128 and 127).\nYou may also get this value doing [2] on the object.\n\n\n\n\nclass Statistics – Statistics Object¶\nThe percentile object is returned by histogram.get_statistics() or\nImage.get_statistics().\nGrayscale statistics have one channel. Use the non l_*, a_*, and\nb_* method.\nRGB565 statistics have three channels. Use the l_*, a_*, and b_*\nmethods.\n\n\nclass image.statistics¶\nPlease call histogram.get_statistics() or Image.get_statistics() to create this object.\n\n\nmean()¶\nReturns the grayscale mean (0-255) (int).\nYou may also get this value doing [0] on the object.\n\n\n\nmedian()¶\nReturns the grayscale median (0-255) (int).\nYou may also get this value doing [1] on the object.\n\n\n\nmode()¶\nReturns the grayscale mode (0-255) (int).\nYou may also get this value doing [2] on the object.\n\n\n\nstdev()¶\nReturns the grayscale standard deviation (0-255) (int).\nYou may also get this value doing [3] on the object.\n\n\n\nmin()¶\nReturns the grayscale min (0-255) (int).\nYou may also get this value doing [4] on the object.\n\n\n\nmax()¶\nReturns the grayscale max (0-255) (int).\nYou may also get this value doing [5] on the object.\n\n\n\nlq()¶\nReturns the grayscale lower quartile (0-255) (int).\nYou may also get this value doing [6] on the object.\n\n\n\nuq()¶\nReturns the grayscale upper quartile (0-255) (int).\nYou may also get this value doing [7] on the object.\n\n\n\nl_mean()¶\nReturns the RGB565 LAB L mean (0-255) (int).\nYou may also get this value doing [0] on the object.\n\n\n\nl_median()¶\nReturns the RGB565 LAB L median (0-255) (int).\nYou may also get this value doing [1] on the object.\n\n\n\nl_mode()¶\nReturns the RGB565 LAB L mode (0-255) (int).\nYou may also get this value doing [2] on the object.\n\n\n\nl_stdev()¶\nReturns the RGB565 LAB L standard deviation (0-255) (int).\nYou may also get this value doing [3] on the object.\n\n\n\nl_min()¶\nReturns the RGB565 LAB L min (0-255) (int).\nYou may also get this value doing [4] on the object.\n\n\n\nl_max()¶\nReturns the RGB565 LAB L max (0-255) (int).\nYou may also get this value doing [5] on the object.\n\n\n\nl_lq()¶\nReturns the RGB565 LAB L lower quartile (0-255) (int).\nYou may also get this value doing [6] on the object.\n\n\n\nl_uq()¶\nReturns the RGB565 LAB L upper quartile (0-255) (int).\nYou may also get this value doing [7] on the object.\n\n\n\na_mean()¶\nReturns the RGB565 LAB A mean (0-255) (int).\nYou may also get this value doing [8] on the object.\n\n\n\na_median()¶\nReturns the RGB565 LAB A median (0-255) (int).\nYou may also get this value doing [9] on the object.\n\n\n\na_mode()¶\nReturns the RGB565 LAB A mode (0-255) (int).\nYou may also get this value doing [10] on the object.\n\n\n\na_stdev()¶\nReturns the RGB565 LAB A standard deviation (0-255) (int).\nYou may also get this value doing [11] on the object.\n\n\n\na_min()¶\nReturns the RGB565 LAB A min (0-255) (int).\nYou may also get this value doing [12] on the object.\n\n\n\na_max()¶\nReturns the RGB565 LAB A max (0-255) (int).\nYou may also get this value doing [13] on the object.\n\n\n\na_lq()¶\nReturns the RGB565 LAB A lower quartile (0-255) (int).\nYou may also get this value doing [14] on the object.\n\n\n\na_uq()¶\nReturns the RGB565 LAB A upper quartile (0-255) (int).\nYou may also get this value doing [15] on the object.\n\n\n\nb_mean()¶\nReturns the RGB565 LAB B mean (0-255) (int).\nYou may also get this value doing [16] on the object.\n\n\n\nb_median()¶\nReturns the RGB565 LAB B median (0-255) (int).\nYou may also get this value doing [17] on the object.\n\n\n\nb_mode()¶\nReturns the RGB565 LAB B mode (0-255) (int).\nYou may also get this value doing [18] on the object.\n\n\n\nb_stdev()¶\nReturns the RGB565 LAB B standard deviation (0-255) (int).\nYou may also get this value doing [19] on the object.\n\n\n\nb_min()¶\nReturns the RGB565 LAB B min (0-255) (int).\nYou may also get this value doing [20] on the object.\n\n\n\nb_max()¶\nReturns the RGB565 LAB B max (0-255) (int).\nYou may also get this value doing [21] on the object.\n\n\n\nb_lq()¶\nReturns the RGB565 LAB B lower quartile (0-255) (int).\nYou may also get this value doing [22] on the object.\n\n\n\nb_uq()¶\nReturns the RGB565 LAB B upper quartile (0-255) (int).\nYou may also get this value doing [23] on the object.\n\n\n\n\nclass Blob – Blob object¶\nThe blob object is returned by Image.find_blobs().\n\n\nclass image.blob¶\nPlease call Image.find_blobs() to create this object.\n\n\ncorners()¶\nReturns a list of 4 (x,y) tuples of the 4 corners of the object. Corners are\nalways returned in sorted clock-wise order starting from the top left.\n\n\n\nmin_corners()¶\nReturns a list of 4 (x,y) tuples of the 4 corners than bound the min area\nrectangle of the blob. Unlike blob.corners() the min area rectangle corners\ndo not necessarily lie on the blob.\n\n\n\nrect()¶\nReturns a rectangle tuple (x, y, w, h) for use with other image methods\nlike Image.draw_rectangle() of the blob’s bounding box.\n\n\n\nx()¶\nReturns the blob’s bounding box x coordinate (int).\nYou may also get this value doing [0] on the object.\n\n\n\ny()¶\nReturns the blob’s bounding box y coordinate (int).\nYou may also get this value doing [1] on the object.\n\n\n\nw()¶\nReturns the blob’s bounding box w coordinate (int).\nYou may also get this value doing [2] on the object.\n\n\n\nh()¶\nReturns the blob’s bounding box h coordinate (int).\nYou may also get this value doing [3] on the object.\n\n\n\npixels()¶\nReturns the number of pixels that are part of this blob (int).\nYou may also get this value doing [4] on the object.\n\n\n\ncx()¶\nReturns the centroid x position of the blob (int).\nYou may also get this value doing [5] on the object.\n\n\n\ncxf()¶\nReturns the centroid x position of the blob (float).\n\n\n\ncy()¶\nReturns the centroid y position of the blob (int).\nYou may also get this value doing [6] on the object.\n\n\n\ncyf()¶\nReturns the centroid y position of the blob (float).\n\n\n\nrotation()¶\nReturns the rotation of the blob in radians (float). If the blob is like\na pencil or pen this value will be unique for 0-180 degrees. If the blob\nis round this value is not useful.\nYou may also get this value doing [7] on the object.\n\n\n\nrotation_deg()¶\nReturns the rotation of the blob in degrees.\n\n\n\nrotation_rad()¶\nReturns the rotation of the blob in radians. This method is more descriptive\nthan just blob.rotation().\n\n\n\ncode()¶\nReturns a 32-bit binary number with a bit set in it for each color threshold\nthat’s part of this blob. For example, if you passed Image.find_blobs()\nthree color thresholds to look for then bits 0/1/2 may be set for this blob.\nNote that only one bit will be set for each blob unless Image.find_blobs()\nwas called with merge=True. Then its possible for multiple blobs with\ndifferent color thresholds to be merged together. You can use this method\nalong with multiple thresholds to implement color code tracking.\nYou may also get this value doing [8] on the object.\n\n\n\ncount()¶\nReturns the number of blobs merged into this blob. This is 1 unless you\ncalled Image.find_blobs() with merge=True.\nYou may also get this value doing [9] on the object.\n\n\n\nperimeter()¶\nReturns the number of pixels on this blob’s perimeter.\n\n\n\nroundness()¶\nReturns a value between 0 and 1 representing how round the object is. A circle would be a 1.\n\n\n\nelongation()¶\nReturns a value between 0 and 1 representing how long (not round) the object is. A line would be a 1.\n\n\n\narea()¶\nReturns the area of the bounding box around the blob. (w * h).\n\n\n\ndensity()¶\nReturns the density ratio of the blob. This is the number of pixels in the\nblob over its bounding box area. A low density ratio means in general that\nthe lock on the object isn’t very good. The result is between 0 and 1.\n\n\n\nextent()¶\nAlias for blob.density().\n\n\n\ncompactness()¶\nLike blob.density(), but, uses the perimeter of the blob instead to measure\nthe objects density and is thus more accurate. The result is between 0 and 1.\n\n\n\nsolidity()¶\nLike blob.density() but, uses the minimum area rotated rectangle versus the\nbounding rectangle to measure density. The result is between 0 and 1.\n\n\n\nconvexity()¶\nReturns a value between 0 and 1 representing how convex the object is. A square would be 1.\n\n\n\nx_hist_bins()¶\nReturns a histogram of the x axis of all columns in a blob. Bin values are\nscaled between 0 and 1.\n\n\n\ny_hist_bins()¶\nReturns a histogram of the y axis of all the rows in a blob. Bin values are\nscaled between 0 and 1.\n\n\n\nmajor_axis_line()¶\nReturns a line tuple (x1, y1, x2, y2) that can be drawn with Image.draw_line() of the major\naxis of the blob (the line going through the longest side of the min area rectangle).\n\n\n\nminor_axis_line()¶\nReturns a line tuple (x1, y1, x2, y2) that can be drawn with Image.draw_line() of the minor\naxis of the blob (the line going through the shortest side of the min area rectangle).\n\n\n\nenclosing_circle()¶\nReturns a circle tuple (x, y, r) that can be drawn with Image.draw_circle() of\nthe circle that encloses the min area rectangle of a blob.\n\n\n\nenclosed_ellipse()¶\nReturns an ellipse tuple (x, y, rx, ry, rotation) that can be drawn with Image.draw_ellipse()\nof the ellipse that fits inside of the min area rectangle of a blob.\n\n\n\n\nclass Line – Line object¶\nThe line object is returned by Image.find_lines(), Image.find_line_segments(), or Image.get_regression().\n\n\nclass image.line¶\nPlease call Image.find_lines(), Image.find_line_segments(), or Image.get_regression() to create this object.\n\n\nline()¶\nReturns a line tuple (x1, y1, x2, y2) for use with other image methods\nlike Image.draw_line().\n\n\n\nx1()¶\nReturns the line’s p1 x component.\nYou may also get this value doing [0] on the object.\n\n\n\ny1()¶\nReturns the line’s p1 y component.\nYou may also get this value doing [1] on the object.\n\n\n\nx2()¶\nReturns the line’s p2 x component.\nYou may also get this value doing [2] on the object.\n\n\n\ny2()¶\nReturns the line’s p2 y component.\nYou may also get this value doing [3] on the object.\n\n\n\nlength()¶\nReturns the line’s length: sqrt(((x2-x1)^2) + ((y2-y1)^2).\nYou may also get this value doing [4] on the object.\n\n\n\nmagnitude()¶\nReturns the magnitude of the line from the hough transform.\nYou may also get this value doing [5] on the object.\n\n\n\ntheta()¶\nReturns the angle of the line from the hough transform - (0 - 179) degrees.\nYou may also get this value doing [7] on the object.\n\n\n\nrho()¶\nReturns the the rho value for the line from the hough transform.\nYou may also get this value doing [8] on the object.\n\n\n\n\nclass Circle – Circle object¶\nThe circle object is returned by Image.find_circles().\n\n\nclass image.circle¶\nPlease call Image.find_circles() to create this object.\n\n\nx()¶\nReturns the circle’s x position.\nYou may also get this value doing [0] on the object.\n\n\n\ny()¶\nReturns the circle’s y position.\nYou may also get this value doing [1] on the object.\n\n\n\nr()¶\nReturns the circle’s radius.\nYou may also get this value doing [2] on the object.\n\n\n\nmagnitude()¶\nReturns the circle’s magnitude.\nYou may also get this value doing [3] on the object.\n\n\n\n\nclass Rect – Rectangle Object¶\nThe rect object is returned by Image.find_rects().\n\n\nclass image.rect¶\nPlease call Image.find_rects() to create this object.\n\n\ncorners()¶\nReturns a list of 4 (x,y) tuples of the 4 corners of the object. Corners are\nalways returned in sorted clock-wise order starting from the top left.\n\n\n\nrect()¶\nReturns a rectangle tuple (x, y, w, h) for use with other image methods\nlike Image.draw_rectangle() of the rect’s bounding box.\n\n\n\nx()¶\nReturns the rectangle’s top left corner’s x position.\nYou may also get this value doing [0] on the object.\n\n\n\ny()¶\nReturns the rectangle’s top left corner’s y position.\nYou may also get this value doing [1] on the object.\n\n\n\nw()¶\nReturns the rectangle’s width.\nYou may also get this value doing [2] on the object.\n\n\n\nh()¶\nReturns the rectangle’s height.\nYou may also get this value doing [3] on the object.\n\n\n\nmagnitude()¶\nReturns the rectangle’s magnitude.\nYou may also get this value doing [4] on the object.\n\n\n\n\nclass QRCode – QRCode object¶\nThe qrcode object is returned by Image.find_qrcodes().\n\n\nclass image.qrcode¶\nPlease call Image.find_qrcodes() to create this object.\n\n\ncorners()¶\nReturns a list of 4 (x,y) tuples of the 4 corners of the object. Corners are\nalways returned in sorted clock-wise order starting from the top left.\n\n\n\nrect()¶\nReturns a rectangle tuple (x, y, w, h) for use with other image methods\nlike Image.draw_rectangle() of the qrcode’s bounding box.\n\n\n\nx()¶\nReturns the qrcode’s bounding box x coordinate (int).\nYou may also get this value doing [0] on the object.\n\n\n\ny()¶\nReturns the qrcode’s bounding box y coordinate (int).\nYou may also get this value doing [1] on the object.\n\n\n\nw()¶\nReturns the qrcode’s bounding box w coordinate (int).\nYou may also get this value doing [2] on the object.\n\n\n\nh()¶\nReturns the qrcode’s bounding box h coordinate (int).\nYou may also get this value doing [3] on the object.\n\n\n\npayload()¶\nReturns the payload string of the qrcode. E.g. the URL.\nYou may also get this value doing [4] on the object.\n\n\n\nversion()¶\nReturns the version number of the qrcode (int).\nYou may also get this value doing [5] on the object.\n\n\n\necc_level()¶\nReturns the ecc_level of the qrcode (int).\nYou may also get this value doing [6] on the object.\n\n\n\nmask()¶\nReturns the mask of the qrcode (int).\nYou may also get this value doing [7] on the object.\n\n\n\ndata_type()¶\nReturns the data type of the qrcode (int).\nYou may also get this value doing [8] on the object.\n\n\n\neci()¶\nReturns the eci of the qrcode (int). The eci stores the encoding of data\nbytes in the QR Code. If you plan to handling QR Codes that contain more\nthan just standard ASCII text you will need to look at this value.\nYou may also get this value doing [9] on the object.\n\n\n\nis_numeric()¶\nReturns True if the data_type of the qrcode is numeric.\n\n\n\nis_alphanumeric()¶\nReturns True if the data_type of the qrcode is alpha numeric.\n\n\n\nis_binary()¶\nReturns True if the data_type of the qrcode is binary. If you are serious\nabout handling all types of text you need to check the eci if this is True\nto determine the text encoding of the data. Usually, it’s just standard\nASCII, but, it could be UTF8 that has some 2-byte characters in it.\n\n\n\nis_kanji()¶\nReturns True if the data_type of the qrcode is alpha Kanji. If this is True\nthen you’ll need to decode the string yourself as Kanji symbols are 10-bits\nper character and MicroPython has no support to parse this kind of text. The\npayload in this case must be treated as just a large byte array.\n\n\n\n\nclass AprilTag – AprilTag object¶\nThe apriltag object is returned by Image.find_apriltags().\n\n\nclass image.apriltag¶\nPlease call Image.find_apriltags() to create this object.\n\n\ncorners()¶\nReturns a list of 4 (x,y) tuples of the 4 corners of the object. Corners are\nalways returned in sorted clock-wise order starting from the top left.\n\n\n\nrect()¶\nReturns a rectangle tuple (x, y, w, h) for use with other image methods\nlike Image.draw_rectangle() of the apriltag’s bounding box.\n\n\n\nx()¶\nReturns the apriltag’s bounding box x coordinate (int).\nYou may also get this value doing [0] on the object.\n\n\n\ny()¶\nReturns the apriltag’s bounding box y coordinate (int).\nYou may also get this value doing [1] on the object.\n\n\n\nw()¶\nReturns the apriltag’s bounding box w coordinate (int).\nYou may also get this value doing [2] on the object.\n\n\n\nh()¶\nReturns the apriltag’s bounding box h coordinate (int).\nYou may also get this value doing [3] on the object.\n\n\n\nid()¶\nReturns the numeric id of the apriltag.\n\n\nTAG16H5 -> 0 to 29\nTAG25H7 -> 0 to 241\nTAG25H9 -> 0 to 34\nTAG36H10 -> 0 to 2319\nTAG36H11 -> 0 to 586\nARTOOLKIT -> 0 to 511\n\n\nYou may also get this value doing [4] on the object.\n\n\n\nfamily()¶\nReturns the numeric family of the apriltag.\n",
        "BINARY¶\nBINARY ": "BINARY¶\nBINARY (bitmap) pixel format. Each pixel is 1-bit.\n\n",
        "PNG¶\nA PNG image.\n\n": "PNG¶\nA PNG image.\n\n"
    },
    "tf": {
        "classify": "classify(path, img[, roi[, min_scale=1.0[, scale_mul=0.5[, x_overlap=0[, y_overlap=0]]]]])¶\nExecutes the TensorFlow Lite image classification model on the img\nobject and returns a list of tf_classification objects. This method\nexecutes the network multiple times on the image in a controllable sliding\nwindow type manner (by default the algorithm only executes the network once\non the whole image frame).\npath a path to a .tflite model to execute on your OpenMV Cam’s\ndisk. The model is loaded into memory, executed, and released all in\none function call to save from having to load the model in the\nMicroPython heap. Pass \"person_detection\" to load the built-in\nperson detection model from your OpenMV Cam’s internal flash.\nroi is the region-of-interest rectangle tuple (x, y, w, h). If not\nspecified, it is equal to the image rectangle. Only pixels within the\nroi are operated on.\nmin_scale controls how much scaling is applied to the network. At the\ndefault value the network is not scaled. However, a value of 0.5 would allow\nfor detecting objects 50% in size of the image roi size…\nscale_mul controls how many different scales are tested out. The sliding\nwindow method works by multiplying a default scale of 1 by scale_mul\nwhile the result is over min_scale. The default value of scale_mul,\n0.5, tests out a 50% size reduction per scale change. However, a value of\n0.95 would only be a 5% size reductioin.\nx_overlap controls the percentage of overlap with the next detector\narea of the sliding window. A value of zero means no overlap. A value of\n0.95 would mean 95% overlap.\ny_overlap controls the percentage of overlap with the next detector\narea of the sliding window. A value of zero means no overlap. A value of\n0.95 would mean 95% overlap.\n\n",
        "segment": "segment(path, img[, roi])¶\nExecutes the TensorFlow Lite image segmentation model on the img\nobject and returns a list of grayscale image objects for each\nsegmentation class output channel.\npath a path to a .tflite model to execute on your OpenMV Cam’s\ndisk. The model is loaded into memory, executed, and released all in\none function call to save from having to load the model in the\nMicroPython heap.\nroi is the region-of-interest rectangle tuple (x, y, w, h). If not\nspecified, it is equal to the image rectangle. Only pixels within the\nroi are operated on.\n\n",
        "detect": "detect(path, img[, roi[, thresholds[, invert]]])¶\nExecutes the TensorFlow Lite image segmentation model on the img\nobject and returns a list of image.blob objects for each segmentation\nclass output. E.g. if you have an image that’s segmented into two classes\nthis method will return a list of two lists of blobs that match the requested\nthresholds.\npath a path to a .tflite model to execute on your OpenMV Cam’s\ndisk. The model is loaded into memory, executed, and released all in\none function call to save from having to load the model in the\nMicroPython heap.\nroi is the region-of-interest rectangle tuple (x, y, w, h). If not\nspecified, it is equal to the image rectangle. Only pixels within the\nroi are operated on.\nthresholds must be a list of tuples\n[(lo, hi), (lo, hi), ..., (lo, hi)] defining the ranges of color you\nwant to track. You may pass up to 32 threshold tuples in one call. Each tuple\nneeds to contain two values - a min grayscale value and a max grayscale value.\nOnly pixel regions that fall between these thresholds will be considered.\nFor easy usage this function will automatically fix swapped min and max values.\nIf the tuple is too short the rest of the thresholds are assumed to be at maximum\nrange. If no thresholds are specified they are assumed to be (128, 255) which\nwill detect “active” pixel regions in the segmented images.\ninvert inverts the thresholding operation such that instead of matching\npixels inside of some known color bounds pixels are matched that are outside\nof the known color bounds.\n\n",
        "regression": "regression(path, array)¶\nExecutes the TensorFlow Lite regression model on the passed array of floats and returns\na new array of floats as the result. This method accepts 1D/2D/3D arrays which must match\nthe input shape of the network. Arrays should be organized in [height][width][channel] order.\npath a path to a .tflite model to execute on your OpenMV Cam’s\ndisk. The model is loaded into memory, executed, and released all in\none function call to save from having to load the model in the\nMicroPython heap.\n\n",
        "load": "load(path[, load_to_fb=False])¶\npath a path to a .tflite model to load into memory on the MicroPython heap by default.\nNOTE! The MicroPython heap is only ~50 KB on the OpenMV Cam M7 and ~256 KB on the OpenMV Cam H7.\nPass \"person_detection\" to load the built-in person detection model from your\nOpenMV Cam’s internal flash. This built-in model does not use any Micropython Heap\nas all the weights are stored in flash which is accessible in the same way as RAM.\nload_to_fb if passed as True will instead reserve part of the OpenMV Cam frame buffer\nstack for storing the TensorFlow Lite model. You will get the most efficent execution\nperformance for large models that do not fit on the heap by loading them into frame buffer\nmemory once from disk and then repeatedly executing the model. That said, the frame buffer\nspace used will not be available anymore for other algorithms.\nReturns a tf_model object which can operate on an image.\n\n",
        "free_from_fb": "free_from_fb()¶\nDeallocates a previously allocated tf_model object created with load_to_fb set to True.\nNote that deallocations happen in the reverse order of allocation.\n\n\n\nclass tf_classification – tf classification dection result¶\nThe tf_classification object is returned by tf.classify() or tf_model.classify().\n\nConstructors¶\n\n\nclass tf.tf_classification¶\nPlease call tf.classify() or tf_model.classify() to create this object.\n\nMethods¶\n\n\nrect()¶\nReturns a rectangle tuple (x, y, w, h) for use with image methods\nlike Image.draw_rectangle() of the tf_classification’s bounding box.\n\n\n\nx()¶\nReturns the tf_classification’s bounding box x coordinate (int).\nYou may also get this value doing [0] on the object.\n\n\n\ny()¶\nReturns the tf_classification’s bounding box y coordinate (int).\nYou may also get this value doing [1] on the object.\n\n\n\nw()¶\nReturns the tf_classification’s bounding box w coordinate (int).\nYou may also get this value doing [2] on the object.\n\n\n\nh()¶\nReturns the tf_classification’s bounding box h coordinate (int).\nYou may also get this value doing [3] on the object.\n\n\n\nclassification_output()¶\nReturns a list of the classification label scores. The size of this\nlist is determined by your model output channel size. For example,\nmobilenet outputs a list of 1000 classification scores for all 1000\nclasses understood by mobilenet. Use zip in python to combine\nthe classification score results with classification labels.\nYou may also get this value doing [4] on the object.\n\n\n\n\n\n\nclass tf_model – TensorFlow Model¶\nIf your model size is small enough and you have enough heap or frame buffer space you may wish\nto directly load the model into memory to save from having to load it from disk\neach time you wish to execute it.\n\nConstructors¶\n\n\nclass tf.tf_model¶\nPlease call tf.load() to create the TensorFlow Model object. TensorFlow Model objects allow\nyou to execute a model from RAM versus having to load it from disk repeatedly.\n\nMethods¶\n\n\nlen()¶\nReturns the size in bytes of the model.\n\n\n\nram()¶\nReturns the model’s required free RAM in bytes.\n\n\n\ninput_height()¶\nReturns the input height of the model. You can use this to size your input\nimage height appropriately.\n\n\n\ninput_width()¶\nReturns the input width of the model. You can use this to size your input\nimage width appropriately.\n\n\n\ninput_channels()¶\nReturns the number of input color channels in the model.\n\n\n\ninput_datatype()¶\nReturns the model’s input datatype (this is a string of “uint8”, “int8”, or “float”).\n\n\n\ninput_scale()¶\nReturns the input scale for the model.\n\n\n\ninput_zero_point()¶\nReturns the output zero point for the model.\n\n\n\noutput_height()¶\nReturns the output height of the model. You can use this to size your output\nimage height appropriately.\n\n\n\noutput_width()¶\nReturns the output width of the model. You can use this to size your output\nimage width appropriately.\n\n\n\noutput_channels()¶\nReturns the number of output color channels in the model.\n\n\n\noutput_datatype()¶\nReturns the model’s output datatype (this is a string of “uint8”, “int8”, or “float”).\n\n\n\noutput_scale()¶\nReturns the output scale for the model.\n\n\n\noutput_zero_point()¶\nReturns the output zero point for the model.\n\n\n\nclassify(img[, roi[, min_scale=1.0[, scale_mul=0.5[, x_overlap=0[, y_overlap=0]]]]])¶\nExecutes the TensorFlow Lite image classification model on the img\nobject and returns a list of tf_classification objects. This method\nexecutes the network multiple times on the image in a controllable sliding\nwindow type manner (by default the algorithm only executes the network once\non the whole image frame).\nroi is the region-of-interest rectangle tuple (x, y, w, h). If not\nspecified, it is equal to the image rectangle. Only pixels within the\nroi are operated on.\nmin_scale controls how much scaling is applied to the network. At the\ndefault value the network is not scaled. However, a value of 0.5 would allow\nfor detecting objects 50% in size of the image roi size…\nscale_mul controls how many different scales are tested out. The sliding\nwindow method works by multiplying a default scale of 1 by scale_mul\nwhile the result is over min_scale. The default value of scale_mul,\n0.5, tests out a 50% size reduction per scale change. However, a value of\n0.95 would only be a 5% size reductioin.\nx_overlap controls the percentage of overlap with the next detector\narea of the sliding window. A value of zero means no overlap. A value of\n0.95 would mean 95% overlap.\ny_overlap controls the percentage of overlap with the next detector\narea of the sliding window. A value of zero means no overlap. A value of\n0.95 would mean 95% overlap.\n\n\n\nsegment(img[, roi])¶\nExecutes the TensorFlow Lite image segmentation model on the img\nobject and returns a list of grayscale image objects for each\nsegmentation class output channel.\nroi is the region-of-interest rectangle tuple (x, y, w, h). If not\nspecified, it is equal to the image rectangle. Only pixels within the\nroi are operated on.\n\n\n\ndetect(img[, roi[, thresholds[, invert]]])¶\nExecutes the TensorFlow Lite image segmentation model on the img\nobject and returns a list of image.blob objects for each segmentation\nclass output. E.g. if you have an image that’s segmented into two classes\nthis method will return a list of two lists of blobs that match the requested\nthresholds.\nroi is the region-of-interest rectangle tuple (x, y, w, h). If not\nspecified, it is equal to the image rectangle. Only pixels within the\nroi are operated on.\nthresholds must be a list of tuples\n[(lo, hi), (lo, hi), ..., (lo, hi)] defining the ranges of color you\nwant to track. You may pass up to 32 threshold tuples in one call. Each tuple\nneeds to contain two values - a min grayscale value and a max grayscale value.\nOnly pixel regions that fall between these thresholds will be considered.\nFor easy usage this function will automatically fix swapped min and max values.\nIf the tuple is too short the rest of the thresholds are assumed to be at maximum\nrange. If no thresholds are specified they are assumed to be (128, 255) which\nwill detect “active” pixel regions in the segmented images.\ninvert inverts the thresholding operation such that instead of matching\npixels inside of some known color bounds pixels are matched that are outside\nof the known color bounds.\n\n\n\nregression(array)¶\nExecutes the TensorFlow Lite regression model on the passed array of floats and returns\na new array of floats as the result. This method accepts 1D/2D/3D arrays which must match\nthe input shape of the network. Arrays should be organized in [height][width][channel] order.\n\n\n\n\n\n\n\n\n\n Previous\nNext \n\n\n\n© Copyright - The MicroPython Documentation is Copyright © 2014-2024, Damien P. George, Paul Sokolovsky, and contributors.\nLast updated on 05 Mar 2024.\n\n\nBuilt with Sphinx using a\ntheme\nprovided by Read the Docs.\n\n\n\n\n\n\n\n Language and External Links\n\n\n\n\nLanguage\n\nEnglish\n\n\n中文\n\n\n\n\nExternal links\n\nopenmv.io\n\n\nforums.openmv.io\n\n\ngithub.com/openmv/openmv\n\n\nmicropython.org\n\n\nforum.micropython.org\n\n\ngithub.com/micropython/micropython\n\n\n\n\n\n"
    },
    "audio": {
        "init": "init([channels=2[, frequency=16000[, gain_db=24[, highpass=0.9883]]]])¶\nInitializes the audio module. Must be called first before using the audio module.\nchannels specifies the number of audio channels. May be 1 or 2. Audio samples are\ninterleaved for two audio channels.\nfrequency is the sample frequency to run at. Running at a higher sample frequency results\nin a higher noise flow which means less effective bits per sample. By default audio samples are\n8-bits with 7-bits of effective dynamic range for voice recording.\ngain_db is the microphone gain to apply.\nhighpass is the high pass filter cut off given the target sample frequency.\n\n",
        "deint": "deint()¶\nDeinitializes the audio module.\n\n",
        "start_streaming": "start_streaming(callback)¶\nCalls the callback that takes one argument pcmbuf automatically forever when enough\nPCM samples have accumulated based on the audio module settings.\npcmbuf is a 16-bit array of audio samples sized based on the decimation factor and number\nof channels.\nIn single channel mode audio samples will be 8-bits each filling up the 16-bit array.\nIn dual channel mode audio samples will be 8-bits each in pairs filling up the 16-bit array.\n\n",
        "stop_streaming": "stop_streaming()¶\nStops audio streaming and the callback from being called.\n\n\n\n\n\n\n Previous\nNext \n\n\n\n© Copyright - The MicroPython Documentation is Copyright © 2014-2024, Damien P. George, Paul Sokolovsky, and contributors.\nLast updated on 05 Mar 2024.\n\n\nBuilt with Sphinx using a\ntheme\nprovided by Read the Docs.\n\n\n\n\n\n\n\n Language and External Links\n\n\n\n\nLanguage\n\nEnglish\n\n\n中文\n\n\n\n\nExternal links\n\nopenmv.io\n\n\nforums.openmv.io\n\n\ngithub.com/openmv/openmv\n\n\nmicropython.org\n\n\nforum.micropython.org\n\n\ngithub.com/micropython/micropython\n\n\n\n\n\n"
    },
    "fir": {
        "init": "init([type=-1[, refresh[, resolution]]])¶\nInitializes an attached thermopile shield using I/O pins P4 and P5 (and P0, P1, P2, P3 for fir.FIR_LEPTON)\ntype indicates the type of thermopile shield:\n",
        "deinit": "deinit()¶\nDeinitializes the thermal sensor freeing up resources.\n\n",
        "width": "width()¶\nReturns the width (horizontal resolution) of the thermal sensor in-use:\n",
        "height": "height()¶\nReturns the height (vertical resolution) of the thermal sensor in-use:\n",
        "type": "type()¶\nReturns the type of the thermal sensor in-use:\n",
        "refresh": "refresh()¶\nReturns the current refresh rate set during fir.init() call.\n\n",
        "resolution": "resolution()¶\nReturns the current resolution set during the fir.init() call.\n\n",
        "radiometric": "radiometric()¶\nReturns if the thermal sensor reports accurate temperature readings (True or False). If False\nthis means that the thermal sensor reports relative temperature readings based on its ambient\ntemperature which may not be very accurate.\n\n",
        "register_vsync_cb": "register_vsync_cb(cb)¶\nFor the fir.FIR_LEPTON mode only on the OpenMV Cam Pure Thermal.\nRegisters callback cb to be executed (in interrupt context) whenever the FLIR Lepton\ngenerates a new frame (but, before the frame is received).\nThis nomially triggers at 9 Hz.\ncb takes no arguments.\n\n",
        "register_frame_cb": "register_frame_cb(cb)¶\nFor the fir.FIR_LEPTON mode only on the OpenMV Cam Pure Thermal.\nRegisters callback cb to be executed (in interrupt context) whenever the FLIR Lepton\ngenerates a new frame and the frame is ready to be read via fir.read_ir() or fir.snapshot().\nThis nomially triggers at 9 Hz.\ncb takes no arguments.\nUse this to get an interrupt to schedule reading a frame later with micropython.schedule().\n\n",
        "get_frame_available": "get_frame_available()¶\nReturns True if a frame is available to read by calling fir.read_ir() or fir.snapshot().\n\n",
        "trigger_ffc": "trigger_ffc([timeout=-1])¶\nFor the fir.FIR_LEPTON mode only.\nTriggers the Flat-Field-Correction process on your FLIR Lepton which calibrates the thermal\nimage. This process happens automatically with the sensor. However, you may call this function\nto force the process to happen.\ntimeout if not -1 then how many milliseconds to wait for FFC to complete.\n\n",
        "read_ta": "read_ta()¶\nReturns the ambient temperature (i.e. sensor temperature).\nExample:\nta = fir.read_ta()\n\n\nThe value returned is a float that represents the temperature in celsius.\n\n",
        "read_ir": "read_ir([hmirror=False[, vflip=False[, transpose=False[, timeout=-1]]]])¶\nReturns a tuple containing the ambient temperature (i.e. sensor temperature),\nthe temperature list (width * height), the minimum temperature seen, and\nthe maximum temperature seen.\nhmirror if set to True horizontally mirrors the ir array.\nvflip if set to True vertically flips the ir array.\ntranspose if set to True transposes the ir array.\ntimeout if not -1 then how many milliseconds to wait for the new frame.\nIf you want to rotate an image by multiples of 90 degrees pass the following:\n* vflip=False, hmirror=False, transpose=False -> 0 degree rotation\n* vflip=True,  hmirror=False, transpose=True  -> 90 degree rotation\n* vflip=True,  hmirror=True,  transpose=False -> 180 degree rotation\n* vflip=False, hmirror=True,  transpose=True  -> 270 degree rotation\n\n\nExample:\nta, ir, to_min, to_max = fir.read_ir()\n\n\nThe values returned are floats that represent the temperature in celsius.\n\nNote\nir is a (width * height) list of floats (4-bytes each).\n\n\n",
        "draw_ir": "draw_ir(image, ir[, x[, y[, x_scale=1.0[, y_scale=1.0[, roi=None[, rgb_channel=-1[, alpha=128[, color_palette=image.PALETTE_RAINBOW[, alpha_palette=-1[, hint=0[, scale=(ir_min, ir_max)]]]]]]]]]]])¶\nDraws an ir array on image whose top-left corner starts at location x, y. This method\nautomatically handles rendering the image passed into the correct pixel format for the destination\nimage while also handling clipping seamlessly.\nx_scale controls how much the displayed image is scaled by in the x direction (float). If this\nvalue is negative the image will be flipped horizontally. Note that if y_scale is not specified\nthen it will match x_scale to maintain the aspect ratio.\ny_scale controls how much the displayed image is scaled by in the y direction (float). If this\nvalue is negative the image will be flipped vertically. Note that if x_scale is not specified\nthen it will match x_scale to maintain the aspect ratio.\nroi is the region-of-interest rectangle tuple (x, y, w, h) of the source image to draw. This\nallows you to extract just the pixels in the ROI to scale and draw on the destination image.\nrgb_channel is the RGB channel (0=R, G=1, B=2) to extract from an RGB565 image (if passed)\nand to render onto the destination image. For example, if you pass rgb_channel=1 this will\nextract the green channel of the source RGB565 image and draw that in grayscale on the\ndestination image.\nalpha controls how much of the source image to blend into the destination image. A value of\n256 draws an opaque source image while a value lower than 256 produces a blend between the source\nand destination image. 0 results in no modification to the destination image.\ncolor_palette if not -1 can be image.PALETTE_RAINBOW, image.PALETTE_IRONBOW, or\na 256 pixel in total RGB565 image to use as a color lookup table on the grayscale value of\nwhatever the source image is. This is applied after rgb_channel extraction if used.\nalpha_palette if not -1 can be a 256 pixel in total GRAYSCALE image to use as a alpha\npalette which modulates the alpha value of the source image being drawn at a pixel pixel\nlevel allowing you to precisely control the alpha value of pixels based on their grayscale value.\nA pixel value of 255 in the alpha lookup table is opaque which anything less than 255 becomes\nmore transparent until 0. This is applied after rgb_channel extraction if used.\nhint can be a logical OR of the flags:\n\n\nimage.AREA: Use area scaling when downscaling versus the default of nearest neighbor.\nimage.BILINEAR: Use bilinear scaling versus the default of nearest neighbor scaling.\nimage.BICUBIC: Use bicubic scaling versus the default of nearest neighbor scaling.\nimage.CENTER: Center the image being drawn on the display. This is applied after scaling.\nimage.HMIRROR: Horizontally mirror the image.\nimage.VFLIP: Vertically flip the image.\nimage.TRANSPOSE: Transpose the image (swap x/y).\nimage.EXTRACT_RGB_CHANNEL_FIRST: Do rgb_channel extraction before scaling.\nimage.APPLY_COLOR_PALETTE_FIRST: Apply color palette before scaling.\nimage.SCALE_ASPECT_KEEP: Scale the image being drawn to fit inside the display.\nimage.SCALE_ASPECT_EXPAND: Scale the image being drawn to fill the display (results in cropping)\nimage.SCALE_ASPECT_IGNORE: Scale the image being drawn to fill the display (results in stretching).\nimage.ROTATE_90: Rotate the image by 90 degrees (this is just VFLIP | TRANSPOSE).\nimage.ROTATE_180: Rotate the image by 180 degrees (this is just HMIRROR | VFLIP).\nimage.ROTATE_270: Rotate the image by 270 degrees (this is just HMIRROR | TRANSPOSE).\n\n\nscale is a two value tuple which controls the min and max temperature (in celsius) to scale\nthe ir image. By default it’s equal to the image ir min and ir max.\nIf x/y are not specified the image will be centered in the field of view. If x_scale/y_scale or\nx_size/y_size are not specified the ir array will be scaled to fit on the image.\n\nNote\nTo handle a transposed ir array read_ir remembers if it was called with transposed\nTrue. This is then passed to draw_ir internally. However, you may pass a 3-value tuple\n(w, h, ir) as the ir array instead to use draw_ir to draw any floating point array with\nwidth w and height h.\n\n\n",
        "snapshot": "snapshot([hmirror=False, [vflip=False, [transpose=False, [x_scale=1.0, [y_scale=1.0, [roi=None, [rgb_channel=-1, [alpha=128, [color_palette=fir.PALETTE_RAINBOW, [alpha_palette=None, [hint=0, [scale=(ir_min, ir_max), [pixformat=image.RGB565, [copy_to_fb=False, [timeout=-1]]]]]]]]]]]]]])¶\nWorks like sensor.snapshot() and returns an image object that is either\nimage.GRAYSCALE (grayscale) or image.RGB565 (color). If copy_to_fb is False then\nthe new image is allocated on the MicroPython heap. However, the MicroPython heap is limited\nand may not have space to store the new image if exhausted. Instead, set copy_to_fb to\nTrue to set the frame buffer to the new image making this function work just like sensor.snapshot().\nhmirror if set to True horizontally mirrors the new image.\nvflip if set to True vertically flips the new image.\ntranspose if set to True transposes the new image.\nIf you want to rotate an image by multiples of 90 degrees pass the following:\n* vflip=False, hmirror=False, transpose=False -> 0 degree rotation\n* vflip=True,  hmirror=False, transpose=True  -> 90 degree rotation\n* vflip=True,  hmirror=True,  transpose=False -> 180 degree rotation\n* vflip=False, hmirror=True,  transpose=True  -> 270 degree rotation\n\n\nx_scale controls how much the displayed image is scaled by in the x direction (float). If this\nvalue is negative the image will be flipped horizontally. Note that if y_scale is not specified\nthen it will match x_scale to maintain the aspect ratio.\ny_scale controls how much the displayed image is scaled by in the y direction (float). If this\nvalue is negative the image will be flipped vertically. Note that if x_scale is not specified\nthen it will match x_scale to maintain the aspect ratio.\nroi is the region-of-interest rectangle tuple (x, y, w, h) of the source image to draw. This\nallows you to extract just the pixels in the ROI to scale and draw on the destination image.\nrgb_channel is the RGB channel (0=R, G=1, B=2) to extract from an RGB565 image (if passed)\nand to render onto the destination image. For example, if you pass rgb_channel=1 this will\nextract the green channel of the source RGB565 image and draw that in grayscale on the\ndestination image.\nalpha controls how much of the source image to blend into the destination image. A value of\n256 draws an opaque source image while a value lower than 256 produces a blend between the source\nand destination image. 0 results in no modification to the destination image.\ncolor_palette if not -1 can be image.PALETTE_RAINBOW, image.PALETTE_IRONBOW, or\na 256 pixel in total RGB565 image to use as a color lookup table on the grayscale value of\nwhatever the source image is. This is applied after rgb_channel extraction if used.\nalpha_palette if not -1 can be a 256 pixel in total GRAYSCALE image to use as a alpha\npalette which modulates the alpha value of the source image being drawn at a pixel pixel\nlevel allowing you to precisely control the alpha value of pixels based on their grayscale value.\nA pixel value of 255 in the alpha lookup table is opaque which anything less than 255 becomes\nmore transparent until 0. This is applied after rgb_channel extraction if used.\nhint can be a logical OR of the flags:\n\n\nimage.AREA: Use area scaling when downscaling versus the default of nearest neighbor.\nimage.BILINEAR: Use bilinear scaling versus the default of nearest neighbor scaling.\nimage.BICUBIC: Use bicubic scaling versus the default of nearest neighbor scaling.\nimage.CENTER: Center the image being drawn on the display. This is applied after scaling.\nimage.HMIRROR: Horizontally mirror the image.\nimage.VFLIP: Vertically flip the image.\nimage.TRANSPOSE: Transpose the image (swap x/y).\nimage.EXTRACT_RGB_CHANNEL_FIRST: Do rgb_channel extraction before scaling.\nimage.APPLY_COLOR_PALETTE_FIRST: Apply color palette before scaling.\nimage.SCALE_ASPECT_KEEP: Scale the image being drawn to fit inside the display.\nimage.SCALE_ASPECT_EXPAND: Scale the image being drawn to fill the display (results in cropping)\nimage.SCALE_ASPECT_IGNORE: Scale the image being drawn to fill the display (results in stretching).\nimage.ROTATE_90: Rotate the image by 90 degrees (this is just VFLIP | TRANSPOSE).\nimage.ROTATE_180: Rotate the image by 180 degrees (this is just HMIRROR | VFLIP).\nimage.ROTATE_270: Rotate the image by 270 degrees (this is just HMIRROR | TRANSPOSE).\n\n\nscale is a two value tuple which controls the min and max temperature (in celsius) to scale\nthe ir image. By default it’s equal to the image ir min and ir max.\npixformat if specified controls the final image pixel format.\ntimeout if not -1 then how many milliseconds to wait for the new frame.\nReturns an image object.\n\n\n\nConstants¶\n"
    },
    "tv": {
        "init": "init([type=tv.TV_SHIELD[, triple_buffer=False]])¶\nInitializes an attached tv output module.\ntype indicates how the lcd module should be initialized:\n\n",
        "deinit": "deinit()¶\nDeinitializes the tv module, internal/external hardware, and I/O pins.\n\n",
        "width": "width()¶\nReturns 352 pixels. This is the sensor.SIF resolution.\n\n",
        "height": "height()¶\nReturns 240 pixels. This is the sensor.SIF resolution.\n\n",
        "type": "type()¶\nReturns the type of the screen that was set during tv.init().\n\n",
        "triple_buffer": "triple_buffer()¶\nReturns if triple buffering is enabled that was set during tv.init().\n\n",
        "refresh": "refresh()¶\nReturns 60 Hz.\n\n",
        "channel": "channel([channel])¶\nFor the wireless TV shield this sets the broadcast channel between 1-8. If passed without a channel\nargument then this method returns the previously set channel (1-8). Default is channel 8.\n\n",
        "display": "display(image[, x=0[, y=0[, x_scale=1.0[, y_scale=1.0[, roi=None[, rgb_channel=-1[, alpha=256[, color_palette=None[, alpha_palette=None[, hint=0]]]]]]]]]])¶\nDisplays an image whose top-left corner starts at location x, y.\nx_scale controls how much the displayed image is scaled by in the x direction (float). If this\nvalue is negative the image will be flipped horizontally. Note that if y_scale is not specified\nthen it will match x_scale to maintain the aspect ratio.\ny_scale controls how much the displayed image is scaled by in the y direction (float). If this\nvalue is negative the image will be flipped vertically. Note that if x_scale is not specified\nthen it will match x_scale to maintain the aspect ratio.\nroi is the region-of-interest rectangle tuple (x, y, w, h) of the image to display. This\nallows you to extract just the pixels in the ROI to scale.\nrgb_channel is the RGB channel (0=R, G=1, B=2) to extract from an RGB565 image (if passed)\nand to render on the display. For example, if you pass rgb_channel=1 this will\nextract the green channel of the RGB565 image and display that in grayscale.\nalpha controls how opaque the image is. A value of 256 displays an opaque image while a\nvalue lower than 256 produces a black transparent image. 0 results in a perfectly black image.\ncolor_palette if not -1 can be image.PALETTE_RAINBOW, image.PALETTE_IRONBOW, or\na 256 pixel in total RGB565 image to use as a color lookup table on the grayscale value of\nwhatever the input image is. This is applied after rgb_channel extraction if used.\nalpha_palette if not -1 can be a 256 pixel in total GRAYSCALE image to use as a alpha\npalette which modulates the alpha value of the input image being displayed at a pixel pixel\nlevel allowing you to precisely control the alpha value of pixels based on their grayscale value.\nA pixel value of 255 in the alpha lookup table is opaque which anything less than 255 becomes\nmore transparent until 0. This is applied after rgb_channel extraction if used.\nhint can be a logical OR of the flags:\n\n\nimage.AREA: Use area scaling when downscaling versus the default of nearest neighbor.\nimage.BILINEAR: Use bilinear scaling versus the default of nearest neighbor scaling.\nimage.BICUBIC: Use bicubic scaling versus the default of nearest neighbor scaling.\nimage.CENTER: Center the image being drawn on the display. This is applied after scaling.\nimage.HMIRROR: Horizontally mirror the image.\nimage.VFLIP: Vertically flip the image.\nimage.TRANSPOSE: Transpose the image (swap x/y).\nimage.EXTRACT_RGB_CHANNEL_FIRST: Do rgb_channel extraction before scaling.\nimage.APPLY_COLOR_PALETTE_FIRST: Apply color palette before scaling.\nimage.SCALE_ASPECT_KEEP: Scale the image being drawn to fit inside the display.\nimage.SCALE_ASPECT_EXPAND: Scale the image being drawn to fill the display (results in cropping)\nimage.SCALE_ASPECT_IGNORE: Scale the image being drawn to fill the display (results in stretching).\nimage.ROTATE_90: Rotate the image by 90 degrees (this is just VFLIP | TRANSPOSE).\nimage.ROTATE_180: Rotate the image by 180 degrees (this is just HMIRROR | VFLIP).\nimage.ROTATE_270: Rotate the image by 270 degrees (this is just HMIRROR | TRANSPOSE).\n\n\n\n\n\nConstants¶\n"
    },
    "cpufreq": {
        "set_frequency": "set_frequency(supported_frequency)¶\nSets the CPU frequency to a supported frequency in MHz. Peripherals\nfrequencies are not changed. Only the CPU performance.\n\n"
    },
    "buzzer": {
        "freq": "freq(freq)¶\nSets the buzzer frequency independently of the volume.\nfreq any frequency to drive the buzzer at.\n\n",
        "duty": "duty(duty)¶\nSets the buzzer duty cycle independently of the frequency.\nduty any PWM duty cycle percentage (0-255 for 0-100%).\n\n\n\nConstants¶\n"
    },
    "imu": {
        "acceleration_mg": "acceleration_mg()¶\nReturns the acceleration for (x, y, z) in a float tuple in milli-g’s.\nFor when the camera board is lying on a table face up:\nX points to the right of the camera sensor\nY points down below the camera sensor (towards the bottom on the board)\nZ points in the reverse direction of the camera sensor (into the table)\n\n",
        "angular_rate_mdps": "angular_rate_mdps()¶\nReturns the angular rate for (x, y, z) in a float tuple in milli-degrees-per-second.\nFor when the camera board is lying on a table face up:\nX points to the right of the camera sensor\nY points down below the camera sensor (towards the bottom on the board)\nZ points in the reverse direction of the camera sensor (into the table)\n\n",
        "temperature_c": "temperature_c()¶\nReturns the temperature in celsius (float).\n\n",
        "roll": "roll()¶\nReturns the rotation angle in degrees (float) of the camera module.\n\n\n0 -> Camera is standing up.\n90 -> Camera is roated left.\n180 -> Camera is upside down.\n270 -> Camera is rotated right.\n\n\n\n",
        "pitch": "pitch()¶\nReturns the rotation angle in degrees (float) of the camera module.\n\n\n0 -> Camera is standing up.\n90 -> Camera is pointing down.\n180 -> Camera is upside down.\n270 -> Camera is pointing up.\n\n\n\n",
        "sleep": "sleep(enable)¶\nPass True to put the IMU sensor to sleep. False to wake it back up (the default).\n\n",
        "__write_reg": "__write_reg(addr, val)¶\nSet 8-bit LSM6DS3 register addr to 8-bit val.\n\n",
        "__read_reg": "__read_reg(addr)¶\nGet 8-bit LSM6DS3 register addr.\n\n\n\n\n\n\n Previous\nNext \n\n\n\n© Copyright - The MicroPython Documentation is Copyright © 2014-2024, Damien P. George, Paul Sokolovsky, and contributors.\nLast updated on 05 Mar 2024.\n\n\nBuilt with Sphinx using a\ntheme\nprovided by Read the Docs.\n\n\n\n\n\n\n\n Language and External Links\n\n\n\n\nLanguage\n\nEnglish\n\n\n中文\n\n\n\n\nExternal links\n\nopenmv.io\n\n\nforums.openmv.io\n\n\ngithub.com/openmv/openmv\n\n\nmicropython.org\n\n\nforum.micropython.org\n\n\ngithub.com/micropython/micropython\n\n\n\n\n\n"
    },
    "omv": {
        "version_major": "version_major()¶\nReturns the major version number (int).\n\n",
        "version_minor": "version_minor()¶\nReturns the minor version number (int).\n\n",
        "version_patch": "version_patch()¶\nReturns the patch version number (int).\n\n",
        "version_string": "version_string()¶\nReturns the version string (e.g. “2.8.0”).\n\n",
        "arch": "arch()¶\nReturns the board architecture string. This string is really just meant for\nOpenMV IDE but you can get it with this function.\n\n",
        "board_type": "board_type()¶\nReturns the board type string. This string is really just meant for\nOpenMV IDE but you can get it with this function.\n\n",
        "board_id": "board_id()¶\nReturns the board id string. This string is really just meant for\nOpenMV IDE but you can get it with this function.\n\n",
        "disable_fb": "disable_fb([disable])¶\nWhen disable is set to True the OpenMV Cam will no longer jpeg compress images and stream\nthem to OpenMV IDE. The IDE may still poll for images unless Disable FB is checked in OpenMV\nIDE. You may wish to disable the frame buffer when streaming images over to another system while\ndebugging you script with OpenMV IDE. If no arguments are passed this function will return\nTrue if the frame buffer is disabled and False if not.\n\nNote\nThis is a different flag than the Disable FB button in OpenMV IDE.\n\n\n\n\n\n\n\n Previous\nNext \n\n\n\n© Copyright - The MicroPython Documentation is Copyright © 2014-2024, Damien P. George, Paul Sokolovsky, and contributors.\nLast updated on 05 Mar 2024.\n\n\nBuilt with Sphinx using a\ntheme\nprovided by Read the Docs.\n\n\n\n\n\n\n\n Language and External Links\n\n\n\n\nLanguage\n\nEnglish\n\n\n中文\n\n\n\n\nExternal links\n\nopenmv.io\n\n\nforums.openmv.io\n\n\ngithub.com/openmv/openmv\n\n\nmicropython.org\n\n\nforum.micropython.org\n\n\ngithub.com/micropython/micropython\n\n\n\n\n\n"
    },
    "math": {
        "acos": "acos(x)¶\nReturn the inverse cosine of x.\n\n",
        "acosh": "acosh(x)¶\nReturn the inverse hyperbolic cosine of x.\n\n",
        "asin": "asin(x)¶\nReturn the inverse sine of x.\n\n",
        "asinh": "asinh(x)¶\nReturn the inverse hyperbolic sine of x.\n\n",
        "atan": "atan(x)¶\nReturn the inverse tangent of x.\n\n",
        "atan2": "atan2(y, x)¶\nReturn the principal value of the inverse tangent of y/x.\n\n",
        "atanh": "atanh(x)¶\nReturn the inverse hyperbolic tangent of x.\n\n",
        "ceil": "ceil(x)¶\nReturn an integer, being x rounded towards positive infinity.\n\n",
        "copysign": "copysign(x, y)¶\nReturn x with the sign of y.\n\n",
        "cos": "cos(x)¶\nReturn the cosine of x.\n\n",
        "cosh": "cosh(x)¶\nReturn the hyperbolic cosine of x.\n\n",
        "degrees": "degrees(x)¶\nReturn radians x converted to degrees.\n\n",
        "erf": "erf(x)¶\nReturn the error function of x.\n\n",
        "erfc": "erfc(x)¶\nReturn the complementary error function of x.\n\n",
        "exp": "exp(x)¶\nReturn the exponential of x.\n\n",
        "expm1": "expm1(x)¶\nReturn exp(x) - 1.\n\n",
        "fabs": "fabs(x)¶\nReturn the absolute value of x.\n\n",
        "floor": "floor(x)¶\nReturn an integer, being x rounded towards negative infinity.\n\n",
        "fmod": "fmod(x, y)¶\nReturn the remainder of x/y.\n\n",
        "frexp": "frexp(x)¶\nDecomposes a floating-point number into its mantissa and exponent.\nThe returned value is the tuple (m, e) such that x == m * 2**e\nexactly. If x == 0 then the function returns (0.0, 0), otherwise\nthe relation 0.5 <= abs(m) < 1 holds.\n\n",
        "gamma": "gamma(x)¶\nReturn the gamma function of x.\n\n",
        "isfinite": "isfinite(x)¶\nReturn True if x is finite.\n\n",
        "isinf": "isinf(x)¶\nReturn True if x is infinite.\n\n",
        "isnan": "isnan(x)¶\nReturn True if x is not-a-number\n\n",
        "ldexp": "ldexp(x, exp)¶\nReturn x * (2**exp).\n\n",
        "lgamma": "lgamma(x)¶\nReturn the natural logarithm of the gamma function of x.\n\n",
        "log": "log(x)¶\nReturn the natural logarithm of x.\n\n",
        "log10": "log10(x)¶\nReturn the base-10 logarithm of x.\n\n",
        "log2": "log2(x)¶\nReturn the base-2 logarithm of x.\n\n",
        "modf": "modf(x)¶\nReturn a tuple of two floats, being the fractional and integral parts of\nx. Both return values have the same sign as x.\n\n",
        "pow": "pow(x, y)¶\nReturns x to the power of y.\n\n",
        "radians": "radians(x)¶\nReturn degrees x converted to radians.\n\n",
        "sin": "sin(x)¶\nReturn the sine of x.\n\n",
        "sinh": "sinh(x)¶\nReturn the hyperbolic sine of x.\n\n",
        "sqrt": "sqrt(x)¶\nReturn the square root of x.\n\n",
        "tan": "tan(x)¶\nReturn the tangent of x.\n\n",
        "tanh": "tanh(x)¶\nReturn the hyperbolic tangent of x.\n\n",
        "trunc": "trunc(x)¶\nReturn an integer, being x rounded towards 0.\n\n\n\nConstants¶\n"
    },
    "time": {
        "gmtime": "gmtime([secs])¶",
        "localtime": "localtime([secs])¶\nConvert the time secs expressed in seconds since the Epoch (see above) into an\n8-tuple which contains: (year, month, mday, hour, minute, second, weekday, yearday)\nIf secs is not provided or None, then the current time from the RTC is used.\nThe gmtime() function returns a date-time tuple in UTC, and localtime() returns a\ndate-time tuple in local time.\nThe format of the entries in the 8-tuple are:\n\nyear includes the century (for example 2014).\nmonth is 1-12\nmday is 1-31\nhour is 0-23\nminute is 0-59\nsecond is 0-59\nweekday is 0-6 for Mon-Sun\nyearday is 1-366\n\n\n",
        "mktime": "mktime()¶\nThis is inverse function of localtime. It’s argument is a full 8-tuple\nwhich expresses a time as per localtime. It returns an integer which is\nthe number of seconds since Jan 1, 2000.\n\n",
        "sleep": "sleep(seconds)¶\nSleep for the given number of seconds. Some boards may accept seconds as a\nfloating-point number to sleep for a fractional number of seconds. Note that\nother boards may not accept a floating-point argument, for compatibility with\nthem use sleep_ms() and sleep_us() functions.\n\n",
        "sleep_ms": "sleep_ms(ms)¶\nDelay for given number of milliseconds, should be positive or 0.\nThis function will delay for at least the given number of milliseconds, but\nmay take longer than that if other processing must take place, for example\ninterrupt handlers or other threads. Passing in 0 for ms will still allow\nthis other processing to occur. Use sleep_us() for more precise delays.\n\n",
        "sleep_us": "sleep_us(us)¶\nDelay for given number of microseconds, should be positive or 0.\nThis function attempts to provide an accurate delay of at least us\nmicroseconds, but it may take longer if the system has other higher priority\nprocessing to perform.\n\n",
        "ticks_ms": "ticks_ms()¶\nReturns an increasing millisecond counter with an arbitrary reference point, that\nwraps around after some value.\nThe wrap-around value is not explicitly exposed, but we will\nrefer to it as TICKS_MAX to simplify discussion. Period of the values is\nTICKS_PERIOD = TICKS_MAX + 1. TICKS_PERIOD is guaranteed to be a power of\ntwo, but otherwise may differ from port to port. The same period value is used\nfor all of ticks_ms(), ticks_us(), ticks_cpu() functions (for\nsimplicity). Thus, these functions will return a value in range [0 ..\nTICKS_MAX], inclusive, total TICKS_PERIOD values. Note that only\nnon-negative values are used. For the most part, you should treat values returned\nby these functions as opaque. The only operations available for them are\nticks_diff() and ticks_add() functions described below.\nNote: Performing standard mathematical operations (+, -) or relational\noperators (<, <=, >, >=) directly on these value will lead to invalid\nresult. Performing mathematical operations and then passing their results\nas arguments to ticks_diff() or ticks_add() will also lead to\ninvalid results from the latter functions.\n\n",
        "ticks_us": "ticks_us()¶\nJust like ticks_ms() above, but in microseconds.\n\n",
        "ticks_cpu": "ticks_cpu()¶\nSimilar to ticks_ms() and ticks_us(), but with the highest possible resolution\nin the system. This is usually CPU clocks, and that’s why the function is named that\nway. But it doesn’t have to be a CPU clock, some other timing source available in a\nsystem (e.g. high-resolution timer) can be used instead. The exact timing unit\n(resolution) of this function is not specified on time module level, but\ndocumentation for a specific port may provide more specific information. This\nfunction is intended for very fine benchmarking or very tight real-time loops.\nAvoid using it in portable code.\nAvailability: Not every port implements this function.\n\n",
        "ticks_add": "ticks_add(ticks, delta)¶\nOffset ticks value by a given number, which can be either positive or negative.\nGiven a ticks value, this function allows to calculate ticks value delta\nticks before or after it, following modular-arithmetic definition of tick values\n(see ticks_ms() above). ticks parameter must be a direct result of call\nto ticks_ms(), ticks_us(), or ticks_cpu() functions (or from previous\ncall to ticks_add()). However, delta can be an arbitrary integer number\nor numeric expression. ticks_add() is useful for calculating deadlines for\nevents/tasks. (Note: you must use ticks_diff() function to work with\ndeadlines.)\nExamples:\n# Find out what ticks value there was 100ms ago\nprint(ticks_add(time.ticks_ms(), -100))\n\n# Calculate deadline for operation and test for it\ndeadline = ticks_add(time.ticks_ms(), 200)\nwhile ticks_diff(deadline, time.ticks_ms()) > 0:\n    do_a_little_of_something()\n\n# Find out TICKS_MAX used by this port\nprint(ticks_add(0, -1))\n\n\n\n",
        "ticks_diff": "ticks_diff(ticks1, ticks2)¶\nMeasure ticks difference between values returned from ticks_ms(), ticks_us(),\nor ticks_cpu() functions, as a signed value which may wrap around.\nThe argument order is the same as for subtraction\noperator, ticks_diff(ticks1, ticks2) has the same meaning as ticks1 - ticks2.\nHowever, values returned by ticks_ms(), etc. functions may wrap around, so\ndirectly using subtraction on them will produce incorrect result. That is why\nticks_diff() is needed, it implements modular (or more specifically, ring)\narithmetic to produce correct result even for wrap-around values (as long as they not\ntoo distant in between, see below). The function returns signed value in the range\n[-TICKS_PERIOD/2 .. TICKS_PERIOD/2-1] (that’s a typical range definition for\ntwo’s-complement signed binary integers). If the result is negative, it means that\nticks1 occurred earlier in time than ticks2. Otherwise, it means that\nticks1 occurred after ticks2. This holds only if ticks1 and ticks2\nare apart from each other for no more than TICKS_PERIOD/2-1 ticks. If that does\nnot hold, incorrect result will be returned. Specifically, if two tick values are\napart for TICKS_PERIOD/2-1 ticks, that value will be returned by the function.\nHowever, if TICKS_PERIOD/2 of real-time ticks has passed between them, the\nfunction will return -TICKS_PERIOD/2 instead, i.e. result value will wrap around\nto the negative range of possible values.\nInformal rationale of the constraints above: Suppose you are locked in a room with no\nmeans to monitor passing of time except a standard 12-notch clock. Then if you look at\ndial-plate now, and don’t look again for another 13 hours (e.g., if you fall for a\nlong sleep), then once you finally look again, it may seem to you that only 1 hour\nhas passed. To avoid this mistake, just look at the clock regularly. Your application\nshould do the same. “Too long sleep” metaphor also maps directly to application\nbehaviour: don’t let your application run any single task for too long. Run tasks\nin steps, and do time-keeping in between.\nticks_diff() is designed to accommodate various usage patterns, among them:\n\nPolling with timeout. In this case, the order of events is known, and you will deal\nonly with positive results of ticks_diff():\n# Wait for GPIO pin to be asserted, but at most 500us\nstart = time.ticks_us()\nwhile pin.value() == 0:\n    if time.ticks_diff(time.ticks_us(), start) > 500:\n        raise TimeoutError\n\n\n\nScheduling events. In this case, ticks_diff() result may be negative\nif an event is overdue:\n# This code snippet is not optimized\nnow = time.ticks_ms()\nscheduled_time = task.scheduled_time()\nif ticks_diff(scheduled_time, now) > 0:\n    print(\"Too early, let's nap\")\n    sleep_ms(ticks_diff(scheduled_time, now))\n    task.run()\nelif ticks_diff(scheduled_time, now) == 0:\n    print(\"Right at time!\")\n    task.run()\nelif ticks_diff(scheduled_time, now) < 0:\n    print(\"Oops, running late, tell task to run faster!\")\n    task.run(run_faster=true)\n\n\n\n\nNote: Do not pass time() values to ticks_diff(), you should use\nnormal mathematical operations on them. But note that time() may (and will)\nalso overflow. This is known as https://en.wikipedia.org/wiki/Year_2038_problem .\n\n",
        "time": "time()¶\nReturns the number of seconds, as an integer, since the Epoch, assuming that\nunderlying RTC is set and maintained as described above. If an RTC is not set, this\nfunction returns number of seconds since a port-specific reference point in time (for\nembedded boards without a battery-backed RTC, usually since power up or reset). If you\nwant to develop portable MicroPython application, you should not rely on this function\nto provide higher than second precision. If you need higher precision, absolute\ntimestamps, use time_ns(). If relative times are acceptable then use the\nticks_ms() and ticks_us() functions. If you need calendar time, gmtime() or\nlocaltime() without an argument is a better choice.\n\nDifference to CPython\nIn CPython, this function returns number of\nseconds since Unix epoch, 1970-01-01 00:00 UTC, as a floating-point,\nusually having microsecond precision. With MicroPython, only Unix port\nuses the same Epoch, and if floating-point precision allows,\nreturns sub-second precision. Embedded hardware usually doesn’t have\nfloating-point precision to represent both long time ranges and subsecond\nprecision, so they use integer value with second precision. Some embedded\nhardware also lacks battery-powered RTC, so returns number of seconds\nsince last power-up or from other relative, hardware-specific point\n(e.g. reset).\n\n\n",
        "time_ns": "time_ns()¶\nSimilar to time() but returns nanoseconds since the Epoch, as an integer (usually\na big integer, so will allocate on the heap).\n\n\n\nConstructors¶\n\n\nclass time.clock¶\nReturns a clock object.\n\nMethods¶\n\n\ntick()¶\nStarts tracking elapsed time.\n\n\n\nfps()¶\nStops tracking the elapsed time and returns the current FPS\n(frames per second).\nAlways call tick first before calling this function.\n\n\n\navg()¶\nStops tracking the elapsed time and returns the current average elapsed time\nin milliseconds.\nAlways call tick first before calling this function.\n\n\n\nreset()¶\nResets the clock object.\n\n\n\n\n\n\n\n\n Previous\nNext \n\n\n\n© Copyright - The MicroPython Documentation is Copyright © 2014-2024, Damien P. George, Paul Sokolovsky, and contributors.\nLast updated on 05 Mar 2024.\n\n\nBuilt with Sphinx using a\ntheme\nprovided by Read the Docs.\n\n\n\n\n\n\n\n Language and External Links\n\n\n\n\nLanguage\n\nEnglish\n\n\n中文\n\n\n\n\nExternal links\n\nopenmv.io\n\n\nforums.openmv.io\n\n\ngithub.com/openmv/openmv\n\n\nmicropython.org\n\n\nforum.micropython.org\n\n\ngithub.com/micropython/micropython\n\n\n\n\n\n"
    },
    "random": {
        "getrandbits": "getrandbits(n)¶\nReturn an integer with n random bits (0 <= n <= 32).\n\n",
        "randint": "randint(a, b)¶\nReturn a random integer in the range [a, b].\n\n",
        "randrange": "randrange(start, stop[, step])\nThe first form returns a random integer from the range [0, stop).\nThe second form returns a random integer from the range [start, stop).\nThe third form returns a random integer from the range [start, stop) in\nsteps of step. For instance, calling randrange(1, 10, 2) will\nreturn odd numbers between 1 and 9 inclusive.\n\n\n\nFunctions for floats¶\n",
        "random": "random()¶\nReturn a random floating point number in the range [0.0, 1.0).\n\n",
        "uniform": "uniform(a, b)¶\nReturn a random floating point number N such that a <= N <= b for a <= b,\nand b <= N <= a for b < a.\n\n\n\nOther Functions¶\n",
        "seed": "seed(n=None, /)¶\nInitialise the random number generator module with the seed n which should\nbe an integer. When no argument (or None) is passed in it will (if\nsupported by the port) initialise the PRNG with a true random number\n(usually a hardware generated random number).\nThe None case only works if MICROPY_PY_RANDOM_SEED_INIT_FUNC is\nenabled by the port, otherwise it raises ValueError.\n\n",
        "choice": "choice(sequence)¶\nChooses and returns one item at random from sequence (tuple, list or\nany object that supports the subscript operation).\n\n\n\n\n\n\n Previous\nNext \n\n\n\n© Copyright - The MicroPython Documentation is Copyright © 2014-2024, Damien P. George, Paul Sokolovsky, and contributors.\nLast updated on 05 Mar 2024.\n\n\nBuilt with Sphinx using a\ntheme\nprovided by Read the Docs.\n\n\n\n\n\n\n\n Language and External Links\n\n\n\n\nLanguage\n\nEnglish\n\n\n中文\n\n\n\n\nExternal links\n\nopenmv.io\n\n\nforums.openmv.io\n\n\ngithub.com/openmv/openmv\n\n\nmicropython.org\n\n\nforum.micropython.org\n\n\ngithub.com/micropython/micropython\n\n\n\n\n\n"
    }
}